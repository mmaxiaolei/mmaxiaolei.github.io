<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[netty源码解读之时间轮算法实现-HashedWheelTimer]]></title>
      <url>%2F2016%2F12%2F02%2Fnetty-hashedwheeltimer%2F</url>
      <content type="text"><![CDATA[前因由于netty动辄管理100w+的连接，每一个连接都会有很多超时任务。比如发送超时、心跳检测间隔等，如果每一个定时任务都启动一个Timer,不仅低效，而且会消耗大量的资源。 解决方案根据George Varghese 和 Tony Lauck 1996 年的论文：Hashed and Hierarchical Timing Wheels: data structures to efficiently implement a timer facility。提出了一种定时轮的方式来管理和维护大量的Timer调度. 原理时间轮其实就是一种环形的数据结构，可以想象成时钟，分成很多格子，一个格子代码一段时间（这个时间越短，Timer的精度越高）。并用一个链表报错在该格子上的到期任务，同时一个指针随着时间一格一格转动，并执行相应格子中的到期任务。任务通过取摸决定放入那个格子。如下图所示： 以上图为例，假设一个格子是1秒，则整个wheel能表示的时间段为8s，假如当前指针指向2，此时需要调度一个3s后执行的任务，显然应该加入到(2+3=5)的方格中，指针再走3次就可以执行了；如果任务要在10s后执行，应该等指针走完一个round零2格再执行，因此应放入4，同时将round（1）保存到任务中。检查到期任务时应当只执行round为0的，格子上其他任务的round应减1。 是不是很像java中的Hashmap。其实就是HashMap的哈希拉链算法，只不过多了指针转动与一些定时处理的逻辑。所以其相关的操作和HashMap也一致： 添加任务：O(1) 删除/取消任务：O(1) 过期/执行任务：最差情况为O(n)-&gt;也就是当HashMap里面的元素全部hash冲突，退化为一条链表的情况。平均O(1)-&gt;显然，格子越多，每个格子上的链表就越短，这里需要权衡时间与空间。 多层时间轮如果任务的时间跨度很大，数量很大，单层的时间轮会造成任务的round很大，单个格子的链表很长。这时候可以将时间轮分层，类似于时钟的时分秒3层。如下图所示： 但是个人认为，多层的时间轮造成的算法复杂度的进一步提升。单层时间轮只需增加每一轮的格子就能解决链表过长的问题。因此，更倾向使用单层的时间轮，netty4中时间轮的实现也是单层的。 netty时间轮的实现-HashedWheelTimer简单使用示例1.引入netty依赖 &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.4.Final&lt;/version&gt; &lt;/dependency&gt; 2.示例代码 示例1： @Test public void test1() throws Exception { DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;); HashedWheelTimer hashedWheelTimer = new HashedWheelTimer(100, TimeUnit.MILLISECONDS); System.out.println(&quot;start:&quot; + LocalDateTime.now().format(formatter)); hashedWheelTimer.newTimeout(timeout -&gt; { System.out.println(&quot;task :&quot; + LocalDateTime.now().format(formatter)); }, 3, TimeUnit.SECONDS); Thread.sleep(5000); } 输出为： start:2016-11-30 05:56:35 task :2016-11-30 05:56:38 示例2： @Test public void test2() throws Exception { DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;); HashedWheelTimer hashedWheelTimer = new HashedWheelTimer(100, TimeUnit.MILLISECONDS); System.out.println(&quot;start:&quot; + LocalDateTime.now().format(formatter)); hashedWheelTimer.newTimeout(timeout -&gt; { Thread.sleep(3000); System.out.println(&quot;task1:&quot; + LocalDateTime.now().format(formatter)); }, 3, TimeUnit.SECONDS); hashedWheelTimer.newTimeout(timeout -&gt; System.out.println(&quot;task2:&quot; + LocalDateTime.now().format( formatter)), 4, TimeUnit.SECONDS); Thread.sleep(10000); } 输出： start:2016-12-01 08:32:37 task1:2016-12-01 08:32:43 task2:2016-12-01 08:32:43 可以看到，当前一个任务执行时间过长的时候，会影响后续任务的到期执行时间的。也就是说其中的任务是串行执行的。所以，要求里面的任务都要短平快。 HashedWheelTimer源码之构造函数public HashedWheelTimer( ThreadFactory threadFactory, // 用来创建worker线程 long tickDuration, // tick的时长，也就是指针多久转一格 TimeUnit unit, // tickDuration的时间单位 int ticksPerWheel, // 一圈有几格 boolean leakDetection // 是否开启内存泄露检测 ) { // 一些参数校验 if (threadFactory == null) { throw new NullPointerException(&quot;threadFactory&quot;); } if (unit == null) { throw new NullPointerException(&quot;unit&quot;); } if (tickDuration &lt;= 0) { throw new IllegalArgumentException(&quot;tickDuration must be greater than 0: &quot; + tickDuration); } if (ticksPerWheel &lt;= 0) { throw new IllegalArgumentException(&quot;ticksPerWheel must be greater than 0: &quot; + ticksPerWheel); } // 创建时间轮基本的数据结构，一个数组。长度为不小于ticksPerWheel的最小2的n次方 wheel = createWheel(ticksPerWheel); // 这是一个标示符，用来快速计算任务应该呆的格子。 // 我们知道，给定一个deadline的定时任务，其应该呆的格子=deadline%wheel.length.但是%操作是个相对耗时的操作，所以使用一种变通的位运算代替： // 因为一圈的长度为2的n次方，mask = 2^n-1后低位将全部是1，然后deadline&amp;mast == deadline%wheel.length // java中的HashMap也是使用这种处理方法 mask = wheel.length - 1; // 转换成纳秒处理 this.tickDuration = unit.toNanos(tickDuration); // 校验是否存在溢出。即指针转动的时间间隔不能太长而导致tickDuration*wheel.length&gt;Long.MAX_VALUE if (this.tickDuration &gt;= Long.MAX_VALUE / wheel.length) { throw new IllegalArgumentException(String.format( &quot;tickDuration: %d (expected: 0 &lt; tickDuration in nanos &lt; %d&quot;, tickDuration, Long.MAX_VALUE / wheel.length)); } // 创建worker线程 workerThread = threadFactory.newThread(worker); // 这里默认是启动内存泄露检测：当HashedWheelTimer实例超过当前cpu可用核数*4的时候，将发出警告 leak = leakDetection || !workerThread.isDaemon() ? leakDetector.open(this) : null; } 再来看下createWheel的代码： private static HashedWheelBucket[] createWheel(int ticksPerWheel) { // 一些参数校验 if (ticksPerWheel &lt;= 0) { throw new IllegalArgumentException( &quot;ticksPerWheel must be greater than 0: &quot; + ticksPerWheel); } if (ticksPerWheel &gt; 1073741824) { throw new IllegalArgumentException( &quot;ticksPerWheel may not be greater than 2^30: &quot; + ticksPerWheel); } // 初始化ticksPerWheel的值为不小于ticksPerWheel的最小2的n次方 ticksPerWheel = normalizeTicksPerWheel(ticksPerWheel); // 初始化wheel数组 HashedWheelBucket[] wheel = new HashedWheelBucket[ticksPerWheel]; for (int i = 0; i &lt; wheel.length; i ++) { wheel[i] = new HashedWheelBucket(); } return wheel; } normalizeTicksPerWheel()的代码： // 初始化ticksPerWheel的值为不小于ticksPerWheel的最小2的n次方 private static int normalizeTicksPerWheel(int ticksPerWheel) { int normalizedTicksPerWheel = 1; while (normalizedTicksPerWheel &lt; ticksPerWheel) { normalizedTicksPerWheel &lt;&lt;= 1; } return normalizedTicksPerWheel; } 这里其实不建议使用这种方式，因为当ticksPerWheel的值很大的时候，这个方法会循环很多次，方法执行时间不稳定，效率也不够。推荐使用java8 HashMap的做法： private int normalizeTicksPerWheel(int ticksPerWheel) { // 这里参考java8 hashmap的算法，使推算的过程固定 int n = ticksPerWheel - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; // 这里1073741824 = 2^30,防止溢出 return (n &lt; 0) ? 1 : (n &gt;= 1073741824) ? 1073741824 : n + 1; } HashedWheelTimer源码之启动、停止与添加任务start()启动时间轮的方法： // 启动时间轮。这个方法其实不需要显示的主动调用，因为在添加定时任务（newTimeout()方法）的时候会自动调用此方法。 // 这个是合理的设计，因为如果时间轮里根本没有定时任务，启动时间轮也是空耗资源 public void start() { // 判断当前时间轮的状态，如果是初始化，则启动worker线程，启动整个时间轮；如果已经启动则略过；如果是已经停止，则报错 // 这里是一个Lock Free的设计。因为可能有多个线程调用启动方法，这里使用AtomicIntegerFieldUpdater原子的更新时间轮的状态 switch (WORKER_STATE_UPDATER.get(this)) { case WORKER_STATE_INIT: if (WORKER_STATE_UPDATER.compareAndSet(this, WORKER_STATE_INIT, WORKER_STATE_STARTED)) { workerThread.start(); } break; case WORKER_STATE_STARTED: break; case WORKER_STATE_SHUTDOWN: throw new IllegalStateException(&quot;cannot be started once stopped&quot;); default: throw new Error(&quot;Invalid WorkerState&quot;); } // 等待worker线程初始化时间轮的启动时间 while (startTime == 0) { try { startTimeInitialized.await(); } catch (InterruptedException ignore) { // Ignore - it will be ready very soon. } } } AtomicIntegerFieldUpdater是JUC里面的类，原理是利用反射进行原子操作。有比AtomicInteger更好的性能和更低得内存占用。跟踪这个类的github 提交记录，可以看到更详细的原因 stop()停止时间轮的方法： public Set&lt;Timeout&gt; stop() { // worker线程不能停止时间轮，也就是加入的定时任务，不能调用这个方法。 // 不然会有恶意的定时任务调用这个方法而造成大量定时任务失效 if (Thread.currentThread() == workerThread) { throw new IllegalStateException( HashedWheelTimer.class.getSimpleName() + &quot;.stop() cannot be called from &quot; + TimerTask.class.getSimpleName()); } // 尝试CAS替换当前状态为“停止：2”。如果失败，则当前时间轮的状态只能是“初始化：0”或者“停止：2”。直接将当前状态设置为“停止：2“ if (!WORKER_STATE_UPDATER.compareAndSet(this, WORKER_STATE_STARTED, WORKER_STATE_SHUTDOWN)) { // workerState can be 0 or 2 at this moment - let it always be 2. WORKER_STATE_UPDATER.set(this, WORKER_STATE_SHUTDOWN); if (leak != null) { leak.close(); } return Collections.emptySet(); } // 终端worker线程 boolean interrupted = false; while (workerThread.isAlive()) { workerThread.interrupt(); try { workerThread.join(100); } catch (InterruptedException ignored) { interrupted = true; } } // 从中断中恢复 if (interrupted) { Thread.currentThread().interrupt(); } if (leak != null) { leak.close(); } // 返回未处理的任务 return worker.unprocessedTimeouts(); } newTimeout()添加定时任务： public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) { // 参数校验 if (task == null) { throw new NullPointerException(&quot;task&quot;); } if (unit == null) { throw new NullPointerException(&quot;unit&quot;); } // 如果时间轮没有启动，则启动 start(); // Add the timeout to the timeout queue which will be processed on the next tick. // During processing all the queued HashedWheelTimeouts will be added to the correct HashedWheelBucket. // 计算任务的deadline long deadline = System.nanoTime() + unit.toNanos(delay) - startTime; // 这里定时任务不是直接加到对应的格子中，而是先加入到一个队列里，然后等到下一个tick的时候，会从队列里取出最多100000个任务加入到指定的格子中 HashedWheelTimeout timeout = new HashedWheelTimeout(this, task, deadline); timeouts.add(timeout); return timeout; } 这里使用的Queue不是普通java自带的Queue的实现，而是使用JCTool–一个高性能的的并发Queue实现包。 HashedWheelTimer源码之HashedWheelTimeoutHashedWheelTimeout是一个定时任务的内部包装类，双向链表结构。会保存定时任务到期执行的任务、deadline、round等信息。 private static final class HashedWheelTimeout implements Timeout { // 定义定时任务的3个状态：初始化、取消、过期 private static final int ST_INIT = 0; private static final int ST_CANCELLED = 1; private static final int ST_EXPIRED = 2; // 用来CAS方式更新定时任务状态 private static final AtomicIntegerFieldUpdater&lt;HashedWheelTimeout&gt; STATE_UPDATER; static { AtomicIntegerFieldUpdater&lt;HashedWheelTimeout&gt; updater = PlatformDependent.newAtomicIntegerFieldUpdater(HashedWheelTimeout.class, &quot;state&quot;); if (updater == null) { updater = AtomicIntegerFieldUpdater.newUpdater(HashedWheelTimeout.class, &quot;state&quot;); } STATE_UPDATER = updater; } // 时间轮引用 private final HashedWheelTimer timer; // 具体到期需要执行的任务 private final TimerTask task; private final long deadline; @SuppressWarnings({&quot;unused&quot;, &quot;FieldMayBeFinal&quot;, &quot;RedundantFieldInitialization&quot; }) private volatile int state = ST_INIT; // 离任务执行的轮数，当将次任务加入到格子中是计算该值，每过一轮，该值减一。 long remainingRounds; // 双向链表结构，由于只有worker线程会访问，这里不需要synchronization / volatile HashedWheelTimeout next; HashedWheelTimeout prev; // 定时任务所在的格子 HashedWheelBucket bucket; HashedWheelTimeout(HashedWheelTimer timer, TimerTask task, long deadline) { this.timer = timer; this.task = task; this.deadline = deadline; } @Override public Timer timer() { return timer; } @Override public TimerTask task() { return task; } @Override public boolean cancel() { // 这里只是修改状态为ST_CANCELLED，会在下次tick时，在格子中移除 if (!compareAndSetState(ST_INIT, ST_CANCELLED)) { return false; } // 加入到时间轮的待取消队列，并在每次tick的时候，从相应格子中移除。 timer.cancelledTimeouts.add(this); return true; } // 从格子中移除自身 void remove() { HashedWheelBucket bucket = this.bucket; if (bucket != null) { bucket.remove(this); } } public boolean compareAndSetState(int expected, int state) { return STATE_UPDATER.compareAndSet(this, expected, state); } public int state() { return state; } @Override public boolean isCancelled() { return state() == ST_CANCELLED; } @Override public boolean isExpired() { return state() == ST_EXPIRED; } // 过期并执行任务 public void expire() { if (!compareAndSetState(ST_INIT, ST_EXPIRED)) { return; } try { task.run(this); } catch (Throwable t) { if (logger.isWarnEnabled()) { logger.warn(&quot;An exception was thrown by &quot; + TimerTask.class.getSimpleName() + &apos;.&apos;, t); } } } // 略过toString() } HashedWheelTimer源码之HashedWheelBucketHashedWheelBucket用来存放HashedWheelTimeout，结构类似于LinkedList。提供了expireTimeouts(long deadline)方法来过期并执行格子中的定时任务 private static final class HashedWheelBucket { // 指向格子中任务的首尾 private HashedWheelTimeout head; private HashedWheelTimeout tail; // 基础的链表添加操作 public void addTimeout(HashedWheelTimeout timeout) { assert timeout.bucket == null; timeout.bucket = this; if (head == null) { head = tail = timeout; } else { tail.next = timeout; timeout.prev = tail; tail = timeout; } } // 过期并执行格子中的到期任务，tick到该格子的时候，worker线程会调用这个方法，根据deadline和remainingRounds判断任务是否过期 public void expireTimeouts(long deadline) { HashedWheelTimeout timeout = head; // 遍历格子中的所有定时任务 while (timeout != null) { boolean remove = false; if (timeout.remainingRounds &lt;= 0) { // 定时任务到期 if (timeout.deadline &lt;= deadline) { timeout.expire(); } else { // 如果round数已经为0，deadline却&gt;当前格子的deadline，说放错格子了，这种情况应该不会出现 throw new IllegalStateException(String.format( &quot;timeout.deadline (%d) &gt; deadline (%d)&quot;, timeout.deadline, deadline)); } remove = true; } else if (timeout.isCancelled()) { remove = true; } else { //没有到期，轮数-1 timeout.remainingRounds --; } // 先保存next，因为移除后next将被设置为null HashedWheelTimeout next = timeout.next; if (remove) { remove(timeout); } timeout = next; } } // 基础的链表移除node操作 public void remove(HashedWheelTimeout timeout) { HashedWheelTimeout next = timeout.next; // remove timeout that was either processed or cancelled by updating the linked-list if (timeout.prev != null) { timeout.prev.next = next; } if (timeout.next != null) { timeout.next.prev = timeout.prev; } if (timeout == head) { // if timeout is also the tail we need to adjust the entry too if (timeout == tail) { tail = null; head = null; } else { head = next; } } else if (timeout == tail) { // if the timeout is the tail modify the tail to be the prev node. tail = timeout.prev; } // null out prev, next and bucket to allow for GC. timeout.prev = null; timeout.next = null; timeout.bucket = null; } /** * Clear this bucket and return all not expired / cancelled {@link Timeout}s. */ public void clearTimeouts(Set&lt;Timeout&gt; set) { for (;;) { HashedWheelTimeout timeout = pollTimeout(); if (timeout == null) { return; } if (timeout.isExpired() || timeout.isCancelled()) { continue; } set.add(timeout); } } // 链表的poll操作 private HashedWheelTimeout pollTimeout() { HashedWheelTimeout head = this.head; if (head == null) { return null; } HashedWheelTimeout next = head.next; if (next == null) { tail = this.head = null; } else { this.head = next; next.prev = null; } // null out prev and next to allow for GC. head.next = null; head.prev = null; head.bucket = null; return head; } } HashedWheelTimer源码之WorkerWorker是时间轮的核心线程类。tick的转动，过期任务的处理都是在这个线程中处理的。 private final class Worker implements Runnable { private final Set&lt;Timeout&gt; unprocessedTimeouts = new HashSet&lt;Timeout&gt;(); private long tick; @Override public void run() { // 初始化startTime.只有所有任务的的deadline都是想对于这个时间点 startTime = System.nanoTime(); // 由于System.nanoTime()可能返回0，甚至负数。并且0是一个标示符，用来判断startTime是否被初始化，所以当startTime=0的时候，重新赋值为1 if (startTime == 0) { startTime = 1; } // 唤醒阻塞在start()的线程 startTimeInitialized.countDown(); // 只要时间轮的状态为WORKER_STATE_STARTED，就循环的“转动”tick，循环判断响应格子中的到期任务 do { // waitForNextTick方法主要是计算下次tick的时间, 然后sleep到下次tick // 返回值就是System.nanoTime() - startTime, 也就是Timer启动后到这次tick, 所过去的时间 final long deadline = waitForNextTick(); if (deadline &gt; 0) { // 可能溢出或者被中断的时候会返回负数, 所以小于等于0不管 // 获取tick对应的格子索引 int idx = (int) (tick &amp; mask); // 移除被取消的任务 processCancelledTasks(); HashedWheelBucket bucket = wheel[idx]; // 从任务队列中取出任务加入到对应的格子中 transferTimeoutsToBuckets(); // 过期执行格子中的任务 bucket.expireTimeouts(deadline); tick++; } } while (WORKER_STATE_UPDATER.get(HashedWheelTimer.this) == WORKER_STATE_STARTED); // 这里应该是时间轮停止了，清除所有格子中的任务，并加入到未处理任务列表，以供stop()方法返回 for (HashedWheelBucket bucket: wheel) { bucket.clearTimeouts(unprocessedTimeouts); } // 将还没有加入到格子中的待处理定时任务队列中的任务取出，如果是未取消的任务，则加入到未处理任务队列中，以供stop()方法返回 for (;;) { HashedWheelTimeout timeout = timeouts.poll(); if (timeout == null) { break; } if (!timeout.isCancelled()) { unprocessedTimeouts.add(timeout); } } // 处理取消的任务 processCancelledTasks(); } // 将newTimeout()方法中加入到待处理定时任务队列中的任务加入到指定的格子中 private void transferTimeoutsToBuckets() { // 每次tick只处理10w个任务，以免阻塞worker线程 for (int i = 0; i &lt; 100000; i++) { HashedWheelTimeout timeout = timeouts.poll(); // 如果没有任务了，直接跳出循环 if (timeout == null) { break; } // 还没有放入到格子中就取消了，直接略过 if (timeout.state() == HashedWheelTimeout.ST_CANCELLED) { continue; } // 计算任务需要经过多少个tick long calculated = timeout.deadline / tickDuration; // 计算任务的轮数 timeout.remainingRounds = (calculated - tick) / wheel.length; //如果任务在timeouts队列里面放久了, 以至于已经过了执行时间, 这个时候就使用当前tick, 也就是放到当前bucket, 此方法调用完后就会被执行. final long ticks = Math.max(calculated, tick); // Ensure we don&apos;t schedule for past. int stopIndex = (int) (ticks &amp; mask); // 将任务加入到响应的格子中 HashedWheelBucket bucket = wheel[stopIndex]; bucket.addTimeout(timeout); } } // 将取消的任务取出，并从格子中移除 private void processCancelledTasks() { for (;;) { HashedWheelTimeout timeout = cancelledTimeouts.poll(); if (timeout == null) { // all processed break; } try { timeout.remove(); } catch (Throwable t) { if (logger.isWarnEnabled()) { logger.warn(&quot;An exception was thrown while process a cancellation task&quot;, t); } } } } /** * calculate goal nanoTime from startTime and current tick number, * then wait until that goal has been reached. * @return Long.MIN_VALUE if received a shutdown request, * current time otherwise (with Long.MIN_VALUE changed by +1) */ //sleep, 直到下次tick到来, 然后返回该次tick和启动时间之间的时长 private long waitForNextTick() { //下次tick的时间点, 用于计算需要sleep的时间 long deadline = tickDuration * (tick + 1); for (;;) { // 计算需要sleep的时间, 之所以加999999后再除10000000, 是为了保证足够的sleep时间 // 例如：当deadline - currentTime=2000002的时候，如果不加999999，则只睡了2ms， // 而2ms其实是未到达deadline这个时间点的，所有为了使上述情况能sleep足够的时间，加上999999后，会多睡1ms final long currentTime = System.nanoTime() - startTime; long sleepTimeMs = (deadline - currentTime + 999999) / 1000000; if (sleepTimeMs &lt;= 0) { // 以下为个人理解：（如有错误，欢迎大家指正） // 这里的意思应该是从时间轮启动到现在经过太长的时间(跨度大于292年...)，以至于让long装不下，都溢出了...对于netty的严谨，我服！ if (currentTime == Long.MIN_VALUE) { return -Long.MAX_VALUE; } else { return currentTime; } } // Check if we run on windows, as if thats the case we will need // to round the sleepTime as workaround for a bug that only affect // the JVM if it runs on windows. // // See https://github.com/netty/netty/issues/356 if (PlatformDependent.isWindows()) { // 这里是因为windows平台的定时调度最小单位为10ms，如果不是10ms的倍数，可能会引起sleep时间不准确 sleepTimeMs = sleepTimeMs / 10 * 10; } try { Thread.sleep(sleepTimeMs); } catch (InterruptedException ignored) { // 调用HashedWheelTimer.stop()时优雅退出 if (WORKER_STATE_UPDATER.get(HashedWheelTimer.this) == WORKER_STATE_SHUTDOWN) { return Long.MIN_VALUE; } } } } public Set&lt;Timeout&gt; unprocessedTimeouts() { return Collections.unmodifiableSet(unprocessedTimeouts); } } 总结通过阅读源码，学到了很多之前不知道的知识点和注意事项。比如： 操作数字型要考虑溢出问题 System.nanoTime(）返回值 Atomic*FieldUpdater类的运用 一些代码设计方式 不断优化性能，Lock Less代替Lock；Lock Free 代码Lock Less JCTool高性能队列的使用]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[spring3的@Async异步执行失效]]></title>
      <url>%2F2016%2F09%2F11%2Fspring3-async%2F</url>
      <content type="text"><![CDATA[背景最近有个项目的spring的@Async的异步执行突然失效了。生产环境异步执行正常，那肯定是开发改了某个地方而导致的。 排查查看最近提交代码记录。与@Async有相关的改动只有一个spring.xml的改动。初步推断是这个改动引起的。回滚这部分代码，跑测试类，果然异步执行生效了。 原因在stackoverflow中找到了答案： In short, the context loaded by the ContextLoaderListener (generally from applicationContext.xml) is the parent of the context loaded by the DispatcherServlet (generally from -servlet.xml). If you have the bean with the @Async method declared/component-scanned in both contexts, the version from the child context (DispatcherServlet) will override the one in the parent context (ContextLoaderListener). I verified this by excluding that component from component scanning in the -servlet.xml – it now works as expected. 意思是说：如果项目中存在多个配置文件（例如：applicationContext.xml、applicationContext-servlet.xml）并且这两个文件中配置的扫描包（即配置的：context:component-scan）都包含了配置过@Async的bean，那么后者就会覆盖前者。 例如以下xml配置: applicationContext.xml: &lt;context:component-scan base-package=&quot;com.demo&quot; /&gt; &lt;task:annotation-driven/&gt; &lt;task:executor id=&quot;executor&quot; pool-size=&quot;5-10&quot; queue-capacity=&quot;100&quot; rejection-policy=&quot;CALLER_RUNS&quot;/&gt; applicationContext-servlet.xml: &lt;context:component-scan base-package=&quot;com.demo&quot; /&gt; 并且在web.xml中配置的加载顺序为：applicationContext.xml &gt; applicationContext-servlet.xml，那么后者的component-scan就会覆盖前者的，同时前者配置的task也会被覆盖掉不起作用！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mac中jdk的路径]]></title>
      <url>%2F2016%2F09%2F08%2Fmac-jdk-dir%2F</url>
      <content type="text"><![CDATA[mac中如何查询jdk安装路径可以使用工具命令/usr/libexec/java_home. 例如以下所示： 还可以加-V选项列出所有的java home. 例如以下所示:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hash碰撞]]></title>
      <url>%2F2016%2F08%2F29%2Fhash-collision%2F</url>
      <content type="text"><![CDATA[Hash定义摘自百度百科： Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入（又叫做预映射， pre-image），通过散列算法，变换成固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。 为什么会产生Hash碰撞 …通过散列算法，变换成固定长度的输出，该输出就是散列值… 既然是根据输入值，变换成固定长度的输出，那就必然会存在不同的输入产生相同的输出。 如何解决Hash碰撞开放地址法这种方法就是在计算一个key的哈希的时候，发现目标地址已经有值了，即发生冲突了，这个时候通过相应的函数在此地址后面的地址去找，直到没有冲突为止。这个方法常用的有线性探测，二次探测，再哈希。这种解决方法有个不好的地方就是，当发生冲突之后，会在之后的地址空间中找一个放进去，这样就有可能后来出现一个key哈希出来的结果也正好是它放进去的这个地址空间，这样就会出现非同义词的两个key发生冲突。 拉链法拉链法是通过数组+链表的形式组合而成的。当发送碰撞后，只需将其追加到对应的链表中即可。如下图所示： java中的HashMap也是采用这种方法解决Hash冲突的。 与开放地址法相比，拉链法有如下优点： 拉链法处理冲突简单，且无堆积现象，即非同义词决不会发生冲突，因此平均查找长度较短 由于链接法中各链表上的结点空间是动态申请的，故它更适合于造表前无法确定表长的情况 开放定址法为减少冲突，要求装填因子α较小，故当结点规模较大时会浪费很多空间。而链接法中可取α≥1，且结点较大时，拉链法中增加的指针域可忽略不计，因此节省空间 用链接法构造的散列表中，删除结点的操作易于实现。只要简单地删去链表上相应的结点即可。而对开放地址法构造的散列表，删除结点不能简单地将被删结点的空间置为空，否则将截断在它之后填入散列表的同义词结点的查找路径。这是因为各种开放地址法中，空地址单元(即开放地址)都是查找失败的条件。因此在 用开放地址法处理冲突的散列表上执行删除操作，只能在被删结点上做删除标记，而不能真正删除结点 拉链法的缺点： 链表指针需要额外空间 以HashMap为例，如果每次插入都产生碰撞，HashMap将退化为一个链表而导致查询的时间复杂度从O(1)变成了O(n)。我们称之为HashMap退化。 P.S. 这里说明下，在jdk8下，HashMap有个改进。当链表的长度大于8时，将转化为一棵红黑树存储。这样即使在最差情况下，查找速度也将在O(lgn) Hash碰撞攻击由于存在Hash碰撞，攻击者只要制造大量碰撞的Hash输入。将会造成大量Hash堆积，轻则查询速度缓慢，拖累网站服务。重则内存溢出，网站服务崩溃。 如何进行Hash碰撞攻击Hash碰撞攻击其实就是寻找Hash碰撞的过程。 目前寻找Hash碰撞的方式有以下4中： 相等子串法：针对某些Hash函数具有相同的字符串组合在上下文中相同位置的Hash值都相同的特性来构造碰撞的。比如f（“string1”）=f（“string2”），那么字符串“aaastring1bbb”与字符串“aaastring2bbb”中，“string1”与“string2”具有相同的Hash值。针对这个特性我们可以构造任意多的碰撞，比如“Ly”和“nz”的Hash值相同，那么“LyLy”、“nznz”、“Lynz”、“nzLy”的Hash值都相同。 生日攻击法：生日攻击方法没有利用Hash函数的结构和任何代数弱性质，它只依赖于消息摘要的长度，即Hash值的长度。这种攻击对Hash函数提出了一个必要的安全条件，即消息摘要必须足够长。生日攻击(传送门)这个术语来自于所谓的生日悖论（传送门）：在一个教室中最少应有多少学生才使得至少有两个学生的生日在同一天的概率不小于1/2？这个问题的答案为23。 中间相遇法：生日攻击的一种变形，它不比较Hash值，而是比较链中的中间变量。这种攻击主要适用于攻击具有分组链结构的Hash方案。中间相遇攻击的基本原理为：将消息分成两部分，对伪造消息的第一部分从初试值开始逐步向中间阶段产生r1个变量；对伪造消息的第二部分从Hash结果开始逐步退回中间阶段产生r2个变量。在中间阶段有一个匹配的概率与生日攻击成功的概率一样。 模差分法：比较高效。应该是目前效率最好的一种方法。传送门 如何抵御Hash碰撞攻击这时候就需要一个算法良好的Hash函数，使最终的Hash值尽量的分布均匀。 目前主流的是time33算法及其变种。 time33算法代码示例： public int time33(char[] str) { int hash = 0; for (char c : str) { hash = hash * 33 + c; } return hash; } 为什么要用33这个倍数因子呢？ 因为1到256之间的所有奇数，都能达到一个可接受的哈希分布，平均分布大概是86%。而其中33，17，31，63，127，129这几个数在面对大量的哈希运算时有一个更大的优势，就是这些数字能将乘法用位运算配合加减法替换，这样运算速度会更高。并不是所有基于time33的算法都使用33作为倍数，如Ngix使用的是time31，Tokyo Cabinet使用的是time37。 P.S. jdk的String.hashCode使用的是time31.而common-lang3中的HashCodeBuilder默认使用的是time37. String.hashCode代码如下： /** * Returns a hash code for this string. The hash code for a * {@code String} object is computed as * &lt;blockquote&gt;&lt;pre&gt; * s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1] * &lt;/pre&gt;&lt;/blockquote&gt; * using {@code int} arithmetic, where {@code s[i]} is the * &lt;i&gt;i&lt;/i&gt;th character of the string, {@code n} is the length of * the string, and {@code ^} indicates exponentiation. * (The hash value of the empty string is zero.) * * @return a hash code value for this object. */ public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[红黑树&TreeMap的实现原理]]></title>
      <url>%2F2016%2F08%2F26%2FtreeMap-theory%2F</url>
      <content type="text"><![CDATA[前言由于TreeMap的实现原理就是以红黑树为基础数据结构的，所以基本也是红黑树的原理解读。 红黑树红黑树是一种自平衡的二叉查找树。是一种复杂但高效的数据结构，它可以在O(log n)时间内做查找，插入和删除。 红黑树的规定： 1.一个节点只能是红色或者黑色2.根节点是黑色3.每个叶节点（null节点/空节点）为黑色4.如果一个节点为红色，则他们的2个子节点都为黑色5.从任意节点到其每个叶节点 红黑树结构java代码示例：（TreeMap中的内部类Entry） /** * 红黑树节点结构 */ static final class Entry&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; { K key; // 红黑树的排序字段 V value; // 节点存储的值 Entry&lt;K, V&gt; left; // 左子树节点 Entry&lt;K, V&gt; right; // 右子树节点 Entry&lt;K, V&gt; parent; // 父节点 boolean color = BLACK; // 节点颜色，默认为黑 /** * Make a new cell with given key, value, and parent, and with * {@code null} child links, and BLACK color. */ Entry(K key, V value, Entry&lt;K, V&gt; parent) { this.key = key; this.value = value; this.parent = parent; } /** * Returns the key. * * @return the key */ public K getKey() { return key; } /** * Returns the value associated with the key. * * @return the value associated with the key */ public V getValue() { return value; } /** * Replaces the value currently associated with the key with the given * value. * * @return the value associated with the key before this method was * called */ public V setValue(V value) { V oldValue = this.value; this.value = value; return oldValue; } public boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?, ?&gt; e = (Map.Entry&lt;?, ?&gt;) o; return valEquals(key, e.getKey()) &amp;&amp; valEquals(value, e.getValue()); } public int hashCode() { int keyHash = (key == null ? 0 : key.hashCode()); int valueHash = (value == null ? 0 : value.hashCode()); return keyHash ^ valueHash; } public String toString() { return key + &quot;=&quot; + value; } } 红黑树的平衡在介绍红黑树的插入前，先弄清楚红黑树如何保持平衡。因为一般在对红黑树插入后，需要对红黑树做平衡化处理。 红黑树的左旋红黑树的左旋操作如下图： 代码如下所示：(TreeMap.rotateLeft) /** * 红黑树左旋 */ private void rotateLeft(Entry&lt;K, V&gt; p) { if (p != null) { // 获取p(对应动图中的E节点)的右子节点定义为r(对应为动图中的S) Entry&lt;K, V&gt; r = p.right; // 将p的右子节点设置为r的左子节点 p.right = r.left; // 如果r的左子节点不为空,则设置r的左子节点的父节点为p if (r.left != null) r.left.parent = p; // 将r的父节点设置为p的父节点 r.parent = p.parent; // 如果p没有父节点,则将r设置为根节点 if (p.parent == null) root = r; // 如果p为p父节点的左子节点,则将p的父节点的左子节点设置为r else if (p.parent.left == p) p.parent.left = r; // 反之将p的父节点的右子结点设置为r else p.parent.right = r; // 将r的左子节点设置为p r.left = p; // 将p的父节点设置为r p.parent = r; } } 红黑树的右旋红黑树的右旋操作如下图： 代码如下所示：（TreeMap.rotateRight） /** * 红黑树右旋 */ private void rotateRight(Entry&lt;K, V&gt; p) { if (p != null) { // 将p的左子节点定义为l Entry&lt;K, V&gt; l = p.left; // 将p的左子节点设置为l的右子节点 p.left = l.right; // 如果l的右子节点不为空,则将l的右子节点的父节点设置为p if (l.right != null) l.right.parent = p; // 将l的父节点设置为p的父节点 l.parent = p.parent; // 如果p的父节点为空,则将l设置为根节点 if (p.parent == null) root = l; // 如果p为p的父节点的右子节点,则将p的父节点的右子节点设置为l else if (p.parent.right == p) p.parent.right = l; // 反之将p的父节点的左子节点设置为l else p.parent.left = l; // 将l的右子节点设置为p l.right = p; // 将p的父节点设置为l p.parent = l; } } 红黑树插入首先红黑树是一棵二叉查找树，所以新增一个节点，首先根据二叉树的性质找到相应的节点位置。然后根据红黑树的特点进行调整和平衡。 红黑树新增节点需要注意以下三点： 1.新增节点默认为红色。2.如果新增的节点的父节点为黑色，那么能维持红黑树的性质。3.如果新增的节点的父节点为红色，那么会破坏红黑树的性质。需要通过重新着色、旋转等手段来维持红黑树的性质。 红黑树的节点新增有以下5种情况：(以下约定,新增节点为N,父节点为P,叔父节点为U,祖父节点为G) 1.新增节点没有父节点，即为跟节点，直接设置为黑色。2.新增节点的父节点为黑色，则直接插入。3.新增节点(N)的父节点(P)和叔父节点(U)都为红色-&gt;将父节点(P)和叔父节点(U)设置为黑，祖父节点(G)设置为红。这时候，由于经由父节点(P)和叔父节点(U)的路径必经过祖父节点(G)，所以这些路径上的黑色节点数目还是相同的。但是祖父节点(G)变为红色之后，祖父节点(G)的父节点可能也是红色，这时候就要将祖父节点(G)当做新增节点递归处理。如下图所示： 4.新增节点(N)的父节点(P)为红色，父节点(P)为祖父节点(G)的左子节点，叔父节点(U)都为黑色或者缺少，且新增节点(N)为父节点(P)的右子节点-&gt;将节点N,P左旋（如下图所所示）。这里注意，如果父节点(P)为祖父节点(G)的右子节点时是要进行右旋。然后产生的结果其实还没有完成，以为违反了规则4，将P节点作为新增节点进行情况5的操作。 5.新增节点(N)的父节点(P)为红色，父节点(P)为祖父节点(G)的左子节点，叔父节点(U)都为黑色或者缺少，且新增节点(N)为父节点(P)的左子节点-&gt;将祖父节(G)与父节点(P)进行右旋（如果父节点(P)为祖父节点(G)的右子节点，进行左旋），然而还没有完成，因为节点P,N都为红色，违反了规则4。将P,G的颜色进行交换。如下图所示： TreeMap中元素的插入TreeMap的put()方法其实只是找到新增节点插入的位置，而插入之后的红黑树平黑调整调用了fixAfterInsertion()方法进行。put()方法代码如下： /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with {@code key}, or * {@code null} if there was no mapping for {@code key}. * (A {@code null} return can also indicate that the map * previously associated {@code null} with {@code key}.) * @throws ClassCastException if the specified key cannot be compared * with the keys currently in the map * @throws NullPointerException if the specified key is null * and this map uses natural ordering, or its comparator * does not permit null keys */ public V put(K key, V value) { // 用t表示当前root节点(也就是整棵数) Entry&lt;K, V&gt; t = root; // 当t为null时,说明是空树,treeMap中没有元素,直接插入 if (t == null) { /* 这里key自我比较很奇怪。 其实是为了key的null校验和k类型检查。 校验key是否可比较的. 这里其实可以用类泛型限定,如TreeMap&lt;key extends Comparable&gt;,之所以没有这么做应该是jdk版本兼容性的考虑 */ compare(key, key); // type (and possibly null) check // 根据key-value,生成节点赋值为root root = new Entry&lt;&gt;(key, value, null); // 容器的元素数量赋值为1 size = 1; // 修改次数+1 modCount++; return null; } int cmp;// key排序比较的结果 Entry&lt;K, V&gt; parent;// 父节点 // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; // 如果指定的比较器不为空,使用指定的比较器进行比较 if (cpr != null) { do { parent = t;//parent赋值为上次循环后的t // 比较key和当前节点的key cmp = cpr.compare(key, t.key); // key小于当前节点的key,则t指向t的左子节点 if (cmp &lt; 0) t = t.left; // key大于当前节点的key,则t指向t的右子节点 else if (cmp &gt; 0) t = t.right; // key等于当前节点的key,直接在当前节点设置新值并返回 else return t.setValue(value); } while (t != null);//递归的进行上述操作,直到t==null } else { // 如果没有知道比较器,则按照默认的比较方式(即自然顺序) // 如果key==null抛出空指针异常 if (key == null) throw new NullPointerException(); // 以下处理过程和上面的一样 @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do { parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); } while (t != null); } // 将新增的节点当做parent的子节点 Entry&lt;K, V&gt; e = new Entry&lt;&gt;(key, value, parent); // 新增节点key小于parent的key,则作为左子节点 if (cmp &lt; 0) parent.left = e; // 新增节点key大于parent的key,则作为右子节点 else parent.right = e; // 新增节点已经在合适的位置了,然后进行红黑树的平衡调整 fixAfterInsertion(e); size++; modCount++; return null; } 再来看看fixAfterInsertion()如何进行红黑树的平衡的，代码如下： /** * 红黑树平衡 */ private void fixAfterInsertion(Entry&lt;K, V&gt; x) { x.color = RED; // 新增节点的颜色设置为红色 // 循环直到x为null或者x是根节点或者x的父节点为黑色 while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) { // 如果x的父节点为x的祖父节点的左节点 if (parentOf(x) == leftOf(parentOf(parentOf(x)))) { // 获取x的叔父节点定义为y Entry&lt;K, V&gt; y = rightOf(parentOf(parentOf(x))); // 如果叔父节点的颜色为红色(情况3) if (colorOf(y) == RED) { // 设置父节点的颜色为黑色 setColor(parentOf(x), BLACK); // 设置叔父节点的颜色为黑色 setColor(y, BLACK); // 设置祖父节点的颜色为红色 setColor(parentOf(parentOf(x)), RED); // x赋值为祖父节点,递归判断 x = parentOf(parentOf(x)); } // 如果叔父节点的颜色为黑色 else { // 如果x为其父节点的右节点(情况4) if (x == rightOf(parentOf(x))) { // x赋值为其父节点 x = parentOf(x); // 根据x的父节点进行左旋 rotateLeft(x); } // 情况5 // 设置x的父节点为黑色 setColor(parentOf(x), BLACK); // 设置x的祖父节点为红色 setColor(parentOf(parentOf(x)), RED); // 根据x的祖父节点右旋 rotateRight(parentOf(parentOf(x))); } } // 如果x的父节点为x的祖父节点的右节点 else { // 定义x的叔父节点为y Entry&lt;K, V&gt; y = leftOf(parentOf(parentOf(x))); // 如果叔父节点的颜色为红色(情况3) if (colorOf(y) == RED) { setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); } // 如果叔父节点的颜色为黑色 else { // 如果x为其父节点的左子节点(情况4) if (x == leftOf(parentOf(x))) { // x赋值为其父节点 x = parentOf(x); // 针对x父节点右旋 rotateRight(x); } // 情况5 // 设置x的父节点颜色为黑色 setColor(parentOf(x), BLACK); // 设置祖父节点颜色为红色 setColor(parentOf(parentOf(x)), RED); // 根据祖父节点进行左旋 rotateLeft(parentOf(parentOf(x))); } } } // 设置根节点颜色为黑色 root.color = BLACK; } 红黑树的删除相比较红黑树的插入，红黑树的删除更加复杂。删除一个节点，一般要寻找一个真正的删除点,替代删除点，然后根据删除点做红黑树的平衡。 如何寻找到真正的删除点呢？其实就是寻找待删除点的中序遍历(LDR)的前继节点或者后继节点。即待删除点的最左父树的右父节点或者右子树的最左节点。 所以可以推断出，真正的删除点必定是一个只有一个孩子或者没有孩子的节点，而根据红黑树的性质，可以得出以下2个结论： 真正的删除点必定只有一个红色孩子节点或者没有孩子节点 如果真正的删除点是一个红色节点，那它必定是个叶子节点 所以，红黑树的删除步骤如下： 1.寻找真正的删除点，将真正删除点的元素赋值为待删除点2.删除真正的删除点，如果删除点有子节点，以子节点代替其位置3.以删除点开始判定红黑树的平衡性质4.如有必要做相应的平衡操作 以下开始讨论红黑树删除的几种情况。我们约定，真正的删除点使用“旧”标记，旧点所在位置将被他的子节点取代（最多只会有一个子节点），我们使用“新”标记旧点的孩子节点。删除操作会有以下集中情况： 1.旧点为红色节点 若旧点为红色节点，则它必定为叶子节点，直接删除即可。 2.一黑一红 当旧点为黑色结点，新点为红色结点时，将新点取代旧点位置后，将新点染成黑色即可（如下图所示）。这里需要注意：旧点为红色，新点为黑色的情况不可能存在。 3.双黑 当旧点和新点都为黑色是（新点为空节点也属于这种情况），情况比较复杂，需要根据旧点兄弟结点的颜色来决定进行什么样的操作。我们使用“兄”来表示旧点的兄弟结点。 3.1 红兄 由于兄弟结点为红色，所以父结点必定为黑色，而旧点被删除后，新点取代了它的位置。下图演示了两种可能的情况： 红兄的情况需要进行RR或LL型旋转，然后将父结点染成红色，兄结点染成黑色。然后重新以新点为判定点进行平衡操作。我们可以观察到，旋转操作完成后，判定点没有向上回溯，而是降低了一层，此时变成了黑兄的情况（可能会是3.2.1、3.2.2、3.2.3）。 3.2 黑兄 黑兄的情况最为复杂，需要根据黑兄孩子结点（这里用“侄”表示）和父亲结点的颜色来决定做什么样的操作。 3.2.1 黑兄二黑侄 这种情况比较简单，只需将兄结点变为红色即可，然后根据父节点继续平衡。（其实这时候如果父节点为红色，将父节点设置为黑色，删除操作就结束了）如下图所示： 3.2.2 黑兄右黑侄 黑兄，左侄红色，右侄黑色这种情况需要区分新点是起父节点的左子节点还是右子节点。 3.2.2-1 新点是其父的左子节点 将左侄设置为黑，兄节点设置为红色，然后以兄节点右旋（将情况转为了3.2.3-1）。如下图所示： 3.2.2-2 新点为其父节点的右子节点 将兄节点的颜色设置为父节点颜色，父节点和左侄节点设置为黑色，然后根据父节点右旋。删除操作结束。如下图所示： 3.2.3 黑兄左黑侄 黑兄，左黑侄，右红侄的情况也是需要区分新点是起父节点的左子节点还是右子节点. 3.2.3-1 新点是左子节点 将兄节点的颜色设置为父节点的颜色，父节点和右侄节点设置为黑色,然后根据父节点左旋。删除结束。如下图所示： 3.2.3-2 新点是右子节点 将右侄设置为黑色，兄节点设置为红色，然后以兄节点左旋（将情况转为3.2.2-2）。如下图说示： TreeMap删除代码TreeMap.remove代码如下： public V remove(Object key) { // 根据key查找出节点p Entry&lt;K, V&gt; p = getEntry(key); // 如果p不存在,直接返回null if (p == null) return null; V oldValue = p.value; // 根据节点p删除节点 deleteEntry(p); return oldValue; } remove其实只是根据key找出对应的节点。真正的删除在deleteEntry方法中,代码如下所示: /** * 删除节点p,然后平衡红黑树. */ private void deleteEntry(Entry&lt;K, V&gt; p) { modCount++; size--; // If strictly internal, copy successor&apos;s element to p and then make p // point to successor. // 如果节点p存在左右子节点,则寻找其真正的删除点(其实是中序遍历的后继节点) if (p.left != null &amp;&amp; p.right != null) { // 寻找中序遍历的后继节点 Entry&lt;K, V&gt; s = successor(p); // 将后继节点的元素值赋给节点p p.key = s.key; p.value = s.value; // 将真正删除点赋值为节点p p = s; } // p has 2 children // Start fixup at replacement node, if it exists. // 定义真正删除点的替代节点 Entry&lt;K, V&gt; replacement = (p.left != null ? p.left : p.right); // 如果存在替代节点 if (replacement != null) { // Link replacement to parent // 将替代节点代替真正删除点的位置 replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; // Null out links so they are OK to use by fixAfterDeletion. // 删除真正删除节点 p.left = p.right = p.parent = null; // Fix replacement // 如果真正删除点为黑色,需要进行删除后的平衡操作 if (p.color == BLACK) fixAfterDeletion(replacement); } // 如果真正删除点的父节点为空,那其实treeMap中只有一个元素,直接删除 else if (p.parent == null) { // return if we are the only node. root = null; } // 如果真正删除点p没有子节点,这里有2中情况,如果p为红色即为情况1,直接删除。反之需要树的平衡 else { // No children. Use self as phantom replacement and unlink. // 如果p节点为黑色,则需要平衡树操作 if (p.color == BLACK) fixAfterDeletion(p); // 删除节点p if (p.parent != null) { if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } } } 在删除节点后，可能需要调用fixAfterDeletion方法来平衡红黑树。fixAfterDeletion方法代码如下所示： /** * 红黑树节点删除后平衡操作 */ private void fixAfterDeletion(Entry&lt;K, V&gt; x) { // 循环平衡红黑树直到根节点或者节点的颜色为红色 while (x != root &amp;&amp; colorOf(x) == BLACK) { // 当节点x为其父节点的左子节点时 if (x == leftOf(parentOf(x))) { // 将x的兄节点定义为sib Entry&lt;K, V&gt; sib = rightOf(parentOf(x)); // 当兄节点为红色时,情况3.1 if (colorOf(sib) == RED) { // 将兄节点设置为黑色 setColor(sib, BLACK); // 将父节点设置为黑色 setColor(parentOf(x), RED); // 以父节点左旋 rotateLeft(parentOf(x)); sib = rightOf(parentOf(x)); } // 如果兄节点的左右子节点都为黑色,情况3.2.1 if (colorOf(leftOf(sib)) == BLACK &amp;&amp; colorOf(rightOf(sib)) == BLACK) { // 将兄节点设置为红色 setColor(sib, RED); // 以x的父节点在递归平衡 x = parentOf(x); } // 如果兄节点左右子节点的颜色不一致 else { // 如果只有右侄节点为黑色,情况3.2.2-1 if (colorOf(rightOf(sib)) == BLACK) { // 将左侄设置为黑色 setColor(leftOf(sib), BLACK); // 将兄节点设置为红色 setColor(sib, RED); // 以兄节点右旋 rotateRight(sib); sib = rightOf(parentOf(x)); } // 情况3.2.3-1 // 将兄节点的颜色设置为父节点的颜色 setColor(sib, colorOf(parentOf(x))); // 将父节点设置为黑色 setColor(parentOf(x), BLACK); // 将右侄节点设置为黑色 setColor(rightOf(sib), BLACK); // 根据父节点左旋 rotateLeft(parentOf(x)); x = root; } } // 当节点x为其父节点的右子节点 else { // symmetric // 定义其兄节点为sib Entry&lt;K, V&gt; sib = leftOf(parentOf(x)); // 当兄节点为红色,情况3.1 if (colorOf(sib) == RED) { // 将兄节点颜色设置为黑色 setColor(sib, BLACK); // 将父节点颜色设置为红色 setColor(parentOf(x), RED); // 以父节点右旋 rotateRight(parentOf(x)); sib = leftOf(parentOf(x)); } // 如果2个侄子节点都是黑色,情况3.2.1 if (colorOf(rightOf(sib)) == BLACK &amp;&amp; colorOf(leftOf(sib)) == BLACK) { // 将兄节点设置为红色 setColor(sib, RED); // 以父节点递归平衡 x = parentOf(x); } // 如果2个侄子节点的颜色不一致 else { // 如果左侄节点颜色为黑色,情况3.2.3-2 if (colorOf(leftOf(sib)) == BLACK) { // 将右侄节点颜色设置为黑色 setColor(rightOf(sib), BLACK); // 将兄节点颜色设置为红色 setColor(sib, RED); // 以兄节点左旋 rotateLeft(sib); sib = leftOf(parentOf(x)); } // 情况3.2.2-2 // 将兄节点颜色设置为父节点颜色 setColor(sib, colorOf(parentOf(x))); // 将父节点颜色设置为黑色 setColor(parentOf(x), BLACK); // 将左侄节点颜色设置为黑色 setColor(leftOf(sib), BLACK); // 以父节点右旋 rotateRight(parentOf(x)); x = root; } } } // 将x节点设置为黑色 setColor(x, BLACK); } 后记这篇笔记写了4天时间。参考了大量大牛的文章，遇到不太懂的时候就拿viso自己画图帮助理解。这次的学习过程收获良多，让我不得不再次感叹数据结构的精妙与算法的魅力。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[chmod777原理]]></title>
      <url>%2F2016%2F08%2F19%2Fwhy-chmod777%2F</url>
      <content type="text"><![CDATA[背景很多Linux新手发现某个文件没有相关权限，一言不合就是chmod 777。首先说一句，chmod 777应该杜绝使用，尤其生产环境。 那chmod 777的背后原理到底是什么呢？ Unix权限设计首先Unix系统的权限分为三种，分别为拥有者(owner)、用户组(group)、其他用户(other)。用ll命令可以查看具体的权限设置，如下图所示： 每个项目前面那一串字母和横杠就是权限。第一位指的是文件类型：-代表普通文件，d代表文件夹。后面9位分为三组，每组代表了对应用户的权限： r=4:读 w=2:写 x=1:执行 回过头来看上图的soft目录，他的权限是drwxr-xr-x就可以解读为： d:这是个目录 rwx:拥有者(onwer)有读、写和执行权限 r-x:用户组（group）有读和执行权限，没有写权限 r-x:其他用户(other)，和用户组的权限一样 读、写、执行的权限值为何是4、2、1?这是因为1、2、4的二进制为： 1:001 2:010 4:100 这么做主要有2个好处：节省空间和提升运算效率。 Unix是上个世纪60年代的产物，当时的硬件资源非常宝贵。所以只用3bit来保存权限。并且二进制的位运算效率特别高，如下例子： public static final int READ = 4； int auth = 5;//101，拥有读和执行权限 /** * 101 (5) * &amp; 100 (4) * = 100 (4) * */ if(auth &amp; READ){ doRead();//有读权限，执行读操作 } 这个权限的判断效率不仅简洁而且高效。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker学习系列七：构建jetty镜像]]></title>
      <url>%2F2016%2F08%2F12%2Fdocker-7-jetty%2F</url>
      <content type="text"><![CDATA[前言Docker hub官方已经维护了一套比较完善的jetty镜像，但是依赖的是openjdk。所以这里只是把jdk换成之前学习系列中构建过的oracle-jdk8。 Dockerfile描述FROM oraclejdk8 MAINTAINER zacard &lt;mmaxiaolei@gmail.com&gt; # add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get added RUN addgroup -S jetty &amp;&amp; adduser -D -S -H -G jetty jetty &amp;&amp; rm -rf /etc/group- /etc/passwd- /etc/shadow- ENV JETTY_HOME /usr/local/jetty ENV PATH $JETTY_HOME/bin:$PATH RUN mkdir -p &quot;$JETTY_HOME&quot; WORKDIR $JETTY_HOME ENV JETTY_BASE /var/lib/jetty RUN mkdir -p &quot;$JETTY_BASE&quot; ENV JETTY_VERSION 9.3.10.v20160621 ENV JETTY_TGZ_URL https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-distribution/$JETTY_VERSION/jetty-distribution-$JETTY_VERSION.tar.gz # GPG Keys are personal keys of Jetty committers (see https://dev.eclipse.org/mhonarc/lists/jetty-users/msg05220.html) ENV JETTY_GPG_KEYS \ # 1024D/8FB67BAC 2006-12-10 Joakim Erdfelt &lt;joakime@apache.org&gt; B59B67FD7904984367F931800818D9D68FB67BAC \ # 1024D/D7C58886 2010-03-09 Jesse McConnell (signing key) &lt;jesse.mcconnell@gmail.com&gt; 5DE533CB43DAF8BC3E372283E7AE839CD7C58886 RUN set -xe \ # Install required packages for build time. Will be removed when build finishes. &amp;&amp; apk add --no-cache --virtual .build-deps gnupg coreutils curl \ &amp;&amp; curl -SL &quot;$JETTY_TGZ_URL&quot; -o jetty.tar.gz \ &amp;&amp; curl -SL &quot;$JETTY_TGZ_URL.asc&quot; -o jetty.tar.gz.asc \ &amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \ &amp;&amp; for key in $JETTY_GPG_KEYS; do \ gpg --keyserver ha.pool.sks-keyservers.net --recv-keys &quot;$key&quot;; done \ &amp;&amp; gpg --batch --verify jetty.tar.gz.asc jetty.tar.gz \ &amp;&amp; rm -r &quot;$GNUPGHOME&quot; \ &amp;&amp; tar -xvzf jetty.tar.gz \ &amp;&amp; mv jetty-distribution-$JETTY_VERSION/* ./ \ &amp;&amp; sed -i &apos;/jetty-logging/d&apos; etc/jetty.conf \ &amp;&amp; rm -fr demo-base javadoc \ &amp;&amp; rm jetty.tar.gz* \ &amp;&amp; rm -fr jetty-distribution-$JETTY_VERSION/ \ # Get the list of modules in the default start.ini and build new base with those modules, then add setuid &amp;&amp; cd $JETTY_BASE \ &amp;&amp; modules=&quot;$(grep -- ^--module= &quot;$JETTY_HOME/start.ini&quot; | cut -d= -f2 | paste -d, -s)&quot; \ &amp;&amp; java -jar &quot;$JETTY_HOME/start.jar&quot; --add-to-startd=&quot;$modules,setuid&quot; \ # Remove installed packages and various cleanup &amp;&amp; apk del .build-deps \ &amp;&amp; rm -fr .build-deps \ &amp;&amp; rm -rf /tmp/hsperfdata_root WORKDIR $JETTY_BASE ENV TMPDIR /tmp/jetty RUN set -xe \ &amp;&amp; mkdir -p &quot;$TMPDIR&quot; \ &amp;&amp; chown -R jetty:jetty &quot;$TMPDIR&quot; &quot;$JETTY_BASE&quot; COPY docker-entrypoint.sh / EXPOSE 8080 ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;] CMD [&quot;java&quot;,&quot;-jar&quot;,&quot;/usr/local/jetty/start.jar&quot;] 说明这里的Dockerfile是根据官方的jetty9-alpine改写。其实也仅仅是把依赖镜像改成了自己构建的oracle-jdk8而已。 这里还依赖了一个sh文件，具体内容如下： docker-entrypoint.sh： #!/bin/sh set -e if [ &quot;$1&quot; = jetty.sh ]; then if ! command -v bash &gt;/dev/null 2&gt;&amp;1 ; then cat &gt;&amp;2 &lt;&lt;- &apos;EOWARN&apos; ******************************************************************** ERROR: bash not found. Use of jetty.sh requires bash. ******************************************************************** EOWARN exit 1 fi cat &gt;&amp;2 &lt;&lt;- &apos;EOWARN&apos; ******************************************************************** WARNING: Use of jetty.sh from this image is deprecated and may be removed at some point in the future. See the documentation for guidance on extending this image: https://github.com/docker-library/docs/tree/master/jetty ******************************************************************** EOWARN fi if ! command -v -- &quot;$1&quot; &gt;/dev/null 2&gt;&amp;1 ; then set -- java -jar &quot;$JETTY_HOME/start.jar&quot; &quot;$@&quot; fi if [ -n &quot;$TMPDIR&quot; ] ; then case &quot;$JAVA_OPTIONS&quot; in *-Djava.io.tmpdir=*) ;; *) JAVA_OPTIONS=&quot;-Djava.io.tmpdir=$TMPDIR $JAVA_OPTIONS&quot; ;; esac fi if [ &quot;$1&quot; = &quot;java&quot; -a -n &quot;$JAVA_OPTIONS&quot; ] ; then shift set -- java $JAVA_OPTIONS &quot;$@&quot; fi exec &quot;$@&quot; 注意：docker-entrypoint.sh要和Dockerfile同一目录，且需要设置可执行权限。 build镜像docker buile -t jetty:9-oraclejdk8 . 测试镜像docker run --rm jetty:9-oraclejdk8 --list-config 启动镜像具体如何启动部署镜像，请参考官方说明文档。传送门 保存镜像docker save -o jetty.tar jetty:9-oraclejdk8 加载镜像docker -i jetty.tar]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java是值传递还是引用传递]]></title>
      <url>%2F2016%2F08%2F09%2Fjava-pass-by-value-or-reference%2F</url>
      <content type="text"><![CDATA[java到底是值传递还是引用传递 “Java manipulates objects ‘by reference,’ but it passes object references to methods ‘by value.’” –David Flanagan 大神解释了：java操作对象都是通过引用传递，而给方法传参都是通过值传递 首先，操作对象通过引用传递这个大家应该没有什么争议。但是方法传参都是通过值传递，估计大家还有些疑虑。 代码验证我们知道，如果是值传递，那么实际上传的是一份拷贝。引用传递的话一般传递的是内存地址（看具体的jvm实现）。所以，值传递是无法修改原值的，而引用传递是可以修改原值。 请看以下代码例子: @Test public void testPassedByValueOrReference() { char[] c1 = {&apos;a&apos;,&apos;b&apos;}; change1(c1); System.out.println(&quot;c1 after change1:&quot; + Arrays.toString(c1)); change2(c1); System.out.println(&quot;c1 after change2:&quot; + Arrays.toString(c1)); } private void change1(char[] c1) { char[] c2 = {&apos;e&apos;,&apos;f&apos;}; c1 = c2; } private void change2(char[] c1) { char[] c2 = {&apos;e&apos;,&apos;f&apos;}; c1[0] = c2[0]; } 输出： c1 after change1:[a, b] c1 after change2:[e, b] 我们知道，数组是个对象。传递给2个不同的方法，确出现了不一致的行为，既有点像值传递，有好像是引用传递。我们来看看这2个方法在内存中到底做了什么： change1()方法在内存中的执行过程: change2()方法在内存中的执行过程: 所以很清楚了，首先，对于方法的传参确实都是值传递，传递的总是拷贝（对象传递的是引用的拷贝，基础类型为原值的拷贝）。当方法中试图改变这个应用所指向的对象时，其实改变的仅仅是这个引用的拷贝，原引用（引用指向的对象）没有任何变化。而当方法改变的是这个引用拷贝所指向的对象内容（假设对象是可变的）时，原引用由于指向的是同一个对象，所以也会受影响。这其实就是java方法有“副作用”的原因。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker学习系列六：构建zookeeper镜像]]></title>
      <url>%2F2016%2F07%2F15%2Fdocker-six-zookeeper%2F</url>
      <content type="text"><![CDATA[什么是zookeeperzookeeper 是一个分布式的，开源的协调服务框架，服务于分布式应用程序。 为什么要zookeeper可以将分布式应用从处理协调服务的泥潭中解救出来。且性能优越，设计简洁优雅。 顺序一致性: 来自客户端的更新操作将会按照顺序被作用 原子性操作: 更新要么全部成功,要么全部失败,没有部分的结果 统一的系统镜像: 无论客户端链接的是哪台服务器,都能获得同样的服务视图,也就是说他是无状态的 可靠性保证: 一旦写入操作被执行(作用到服务器),这个状态将会被持久化,直到其他客户端的修改生效 时间线特性: 客户端访问服务器系统镜像能在一个特定时间访问内保证当前系统是实时更新的 DockfileFROM oraclejdk8 MAINTAINER zacard &lt;mmaxiaolei@gmail.com&gt; # Install required packages RUN apk add --no-cache \ bash \ su-exec ENV ZOO_USER zookeeper ENV ZOO_CONF_DIR /conf ENV ZOO_DATA_DIR /data ENV ZOO_DATA_LOG_DIR /datalog # Add a user and make dirs RUN set -x \ &amp;&amp; adduser -D &quot;$ZOO_USER&quot; \ &amp;&amp; mkdir -p &quot;$ZOO_DATA_LOG_DIR&quot; &quot;$ZOO_DATA_DIR&quot; &quot;$ZOO_CONF_DIR&quot; \ &amp;&amp; chown &quot;$ZOO_USER:$ZOO_USER&quot; &quot;$ZOO_DATA_LOG_DIR&quot; &quot;$ZOO_DATA_DIR&quot; &quot;$ZOO_CONF_DIR&quot; ARG GPG_KEY=C823E3E5B12AF29C67F81976F5CECB3CB5E9BD2D ARG DISTRO_NAME=zookeeper-3.4.9 # Download Apache Zookeeper, verify its PGP signature, untar and clean up RUN set -x \ &amp;&amp; apk add --no-cache --virtual .build-deps \ gnupg \ &amp;&amp; wget -q &quot;http://www.apache.org/dist/zookeeper/$DISTRO_NAME/$DISTRO_NAME.tar.gz&quot; \ &amp;&amp; wget -q &quot;http://www.apache.org/dist/zookeeper/$DISTRO_NAME/$DISTRO_NAME.tar.gz.asc&quot; \ &amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \ &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-key &quot;$GPG_KEY&quot; \ &amp;&amp; gpg --batch --verify &quot;$DISTRO_NAME.tar.gz.asc&quot; &quot;$DISTRO_NAME.tar.gz&quot; \ &amp;&amp; tar -xzf &quot;$DISTRO_NAME.tar.gz&quot; \ &amp;&amp; mv &quot;$DISTRO_NAME/conf/&quot;* &quot;$ZOO_CONF_DIR&quot; \ &amp;&amp; rm -r &quot;$GNUPGHOME&quot; &quot;$DISTRO_NAME.tar.gz&quot; &quot;$DISTRO_NAME.tar.gz.asc&quot; \ &amp;&amp; apk del .build-deps WORKDIR $DISTRO_NAME VOLUME [&quot;$ZOO_DATA_DIR&quot;, &quot;$ZOO_DATA_LOG_DIR&quot;] ENV ZOO_PORT 2181 EXPOSE $ZOO_PORT ENV PATH $PATH:/$DISTRO_NAME/bin ENV ZOOCFGDIR $ZOO_CONF_DIR COPY docker-entrypoint.sh / ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;] CMD [&quot;zkServer.sh&quot;, &quot;start-foreground&quot;] 注意：这里依赖的镜像oraclejdk8请查看之前的文章“Docker学习系列五：构建oracle-jdk8镜像” 依赖的docker-entrypoint.sh#!/bin/bash set -e # Allow the container to be started with `--user` if [ &quot;$1&quot; = &apos;zkServer.sh&apos; -a &quot;$(id -u)&quot; = &apos;0&apos; ]; then exec su-exec &quot;$ZOO_USER&quot; &quot;$0&quot; &quot;$@&quot; fi # Generate the config only if it doesn&apos;t exist if [ ! -f &quot;$ZOO_CONF_DIR/zoo.cfg&quot; ]; then CONFIG=&quot;$ZOO_CONF_DIR/zoo.cfg&quot; echo &quot;clientPort=$ZOO_PORT&quot; &gt;&gt; &quot;$CONFIG&quot; echo &quot;dataDir=$ZOO_DATA_DIR&quot; &gt;&gt; &quot;$CONFIG&quot; echo &quot;dataLogDir=$ZOO_DATA_LOG_DIR&quot; &gt;&gt; &quot;$CONFIG&quot; echo &apos;tickTime=2000&apos; &gt;&gt; &quot;$CONFIG&quot; echo &apos;initLimit=5&apos; &gt;&gt; &quot;$CONFIG&quot; echo &apos;syncLimit=2&apos; &gt;&gt; &quot;$CONFIG&quot; for server in $ZOO_SERVERS; do echo &quot;$server&quot; &gt;&gt; &quot;$CONFIG&quot; done fi # Write myid only if it doesn&apos;t exist if [ ! -f &quot;$ZOO_DATA_DIR/myid&quot; ]; then echo &quot;${ZOO_MY_ID:-1}&quot; &gt; &quot;$ZOO_DATA_DIR/myid&quot; fi exec &quot;$@&quot; 设置docker-entrypoint.sh权限： chmod 755 docker-entrypoint.sh build镜像docker build -t zookeeper:3.4.9 . 测试镜像启动镜像docker run --name zookeeper --restart always -d zookeeper:3.4.9 This image includes EXPOSE 2181 (the zookeeper port), so standard container linking will make it automatically available to the linked containers. Since the Zookeeper “fails fast” it’s better to always restart it. 这个镜像内部开放了2181端口(zookeeper默认端口)，所有标准的容器链接会使之自动可用。然后因为Zookeeper是fail fast，所以最好总是能自动重启。 从另一个应用容器链接到zookeeper容器docker run --name some-app --link some-zookeeper:zookeeper -d application-that-uses-zookeeper 从zookeeper命令行客户端链接到zookeeper容器docker run -it --rm --link some-zookeeper:zookeeper zookeeper zkCli.sh -server zookeeper 集群模式启动zookeeperdocker-compose.yml: version: &apos;2&apos; services: zoo1: image: zookeeper:3.4.9 restart: always ports: - 2181 environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo2: image: zookeeper:3.4.9 restart: always ports: - 2181 environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo3: image: zookeeper:3.4.9 restart: always ports: - 2181 environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 启动集群： docker-compose up 查看集群状态（端口）： docker-compose ps 这里需要注意：这里是伪集群。因为所有容器都启动在同一个物理主机中。实际应该是在不同的主机中启动zookeeper容器。 配置zookeeper的配置在/conf目录下。如果需要修改配置，可以挂载本地配置文件。例如以下所示： docker run --name some-zookeeper --restart always -d -v $(pwd)/zoo.cfg:/conf/zoo.cfg zookeeper]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[创造Lambda风格的mybatis批量操作]]></title>
      <url>%2F2016%2F05%2F31%2Fmybatis-batch-java8%2F</url>
      <content type="text"><![CDATA[mybatis批量操作mybatis如何批量插入呢？常用的做法如下代码所示： // UserDao.java /** * 批量插入 */ int batchInsert(@Param(&quot;users&quot;) List&lt;User&gt; users); // UserDao.xml &lt;insert id=&quot;batchInsert&quot;&gt; INSERT INTO user (name,password) VALUES &lt;foreach collection=&quot;users&quot; item=&quot;user&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; (#{user.name},#{user.password}) &lt;/foreach&gt; &lt;/insert&gt; 但是，这样有个影藏的bug：当user集合超过一定数量，会导致动态拼接的sql过长而导致执行报错。并且，批量更新就不能使用这种形式了。 使用mybatis批量提交方式是否有一种通用的写法实现批量操作呢？查看github中mybatis的官方wiki，里面有批量操作的示例（传送门）: // Mapper.xml &lt;insert id=&quot;insertName&quot;&gt; insert into names (name) values (#{value}) &lt;/insert&gt; // java code List&lt;String&gt; names = new ArrayList&lt;String&gt;(); names.add(&quot;Fred&quot;); names.add(&quot;Barney&quot;); names.add(&quot;Betty&quot;); names.add(&quot;Wilma&quot;); SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.BATCH); try { NameMapper mapper = sqlSession.getMapper(NameMapper.class); for (String name : names) { mapper.insertName(name); } sqlSession.commit(); } finally { sqlSession.close(); } 这种java代码形式的批量操作更加通用且直观。但是需要自己管理sqlsession，这部分自我管理sqlsession的代码不但容易忘记（特别是finally中的sqlsession.close）,而且违反了DRY原则，代码也显得冗长。 使用java8 lambda改造首先对于sqlsession的资源管理可以使用java7的特性，try-with-resources管理。例如以下代码： List&lt;String&gt; names = new ArrayList&lt;String&gt;(); names.add(&quot;Fred&quot;); names.add(&quot;Barney&quot;); names.add(&quot;Betty&quot;); names.add(&quot;Wilma&quot;); try (SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.BATCH)) { NameMapper mapper = sqlSession.getMapper(NameMapper.class); for (String name : names) { mapper.insertName(name); } sqlSession.commit(); } catch (Exception e) { logger.error(&quot;批量操作失败&quot;, e); } 然而，获取sqlsession和sql.commit等部分仍然是重复代码。调用者可能并不关心这些实现细节。 将批量操作独立为一个工具类，并使用java8 lambda改造： /** * MyBatis 批量操作 * * @author zacard * @since 2016-05-31 09:17 */ @Component public class MyBatisBatch { private static final Logger logger = LoggerFactory.getLogger(MyBatisBatch.class); @Autowired private SqlSessionFactory xSqlSessionFactory; public &lt;T&gt; void doBatch(Class&lt;T&gt; daoClass, Consumer&lt;T&gt; consumer){ Objects.requireNonNull(consumer); if (xSqlSessionFactory == null) { logger.error(&quot;无法获取mybatis sqlSessionFactory,请检查mybatis配置&quot;); throw XExceptionFactory.create(&quot;mybatis配置错误&quot;)); } try (SqlSession sqlSession = xSqlSessionFactory.openSession(ExecutorType.BATCH)) { T mapper = sqlSession.getMapper(daoClass); consumer.accept(mapper); sqlSession.commit(); } catch (Exception e) { logger.error(&quot;批量操作失败&quot;, e); throw XExceptionFactory.create(&quot;批量操作失败&quot;); } } 客户端使用示例： List&lt;Long&gt; userIds = new ArrayList&lt;&gt;(); userIds.add(1L); userIds.add(2L); userIds.add(3L); myBatisBatch.doBatch(UserDao.class, userDao -&gt; userIds.forEach(userId -&gt; userDao.insert(userId)) );]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ThreadLocal的作用与原理]]></title>
      <url>%2F2016%2F05%2F23%2Fjava-threadlocal%2F</url>
      <content type="text"><![CDATA[背景spring默认bean的注册形式是单例模式。那spring是如何解决并发安全问题的呢？就是通过ThreadLocal。到底ThreadLocal有和“魔力”能让普通类变成线程安全的类呢？ 原理先来看看ThreadLocal.java的源码注释： This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its {@code get} or {@code set} method) has its own, independently initialized copy of the variable. {@code ThreadLocal} instances are typically private static fields in classes that wish to associate state with a thread (e.g.,a user ID or Transaction ID). 大致意思： 这个类提供线程局部变量。这种在多线程环境下访问（通过get或set方法）时，能保证各个线程里的变量相对独立于其他线程内的变量。ThreadLocal实例通常是private static类型的，用于管理线程上下文。 也就是说，Threadlocal提供了作用范围为线程的局部变量，这种变量只在线程生命周期内起作用。减少了线程内多个方法之间公共变量传递的复杂度。 这里关于线程安全的类有一个普遍适用的原则：如果一个类没有实例私有属性，或者实例私有属性也是无状态的类，那么这个类就是无状态的类。而一个无状态的类肯定是线程安全的类。 而用ThreadLocal包装类的所有实例私有属性后，这个类就没有实例私有属性了，那么这个类就是一个无状态类，因此也是一个线程安全的类。这也是spring使用ThreeadLocal处理bean的默认方式。 ThreadLocal基本使用先来看看ThreadLocal几个常用方法。 构造函数/** * Creates a thread local variable. * @see #withInitial(java.util.function.Supplier) */ public ThreadLocal() { } 内部没有任何实现。 initialValue方法protected T initialValue() { return null; } initialValue()用来设置ThreadLocal的初始值。方法是protected的，建议在子类中被重载，以指定初始值。通常使用匿名内部类的形式。例如以下代码所示： /** * ThreadLocal测试类 * * @author zacard * @since 2016-05-23 16:36 */ public class ThreadLocalTest { private static final AtomicInteger nextId = new AtomicInteger(0); ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;Integer&gt;() { @Override protected Integer initialValue() { return nextId.getAndIncrement(); } }; } withInitialwithInitial（）也是用来初始化的，但是是lamda风格的初始化方法。构造方法中也是推荐使用此方法。例如以下代码所示： /** * ThreadLocal测试类 * * @author zacard * @since 2016-05-23 16:36 */ public class ThreadLocalTest { private static final AtomicInteger nextId = new AtomicInteger(0); ThreadLocal&lt;Integer&gt; threadLocal = ThreadLocal.withInitial(nextId::getAndIncrement); } 使用测试代码/** * ThreadLocal测试类 * * @author zacard * @since 2016-05-23 16:36 */ public class ThreadLocalTest { private static final AtomicInteger nextId = new AtomicInteger(0); private static final ThreadLocal&lt;Integer&gt; threadLocal = ThreadLocal.withInitial(nextId::getAndIncrement); public static void main(String[] args) { for (int i = 0; i &lt; 3; i++) { new Thread(new Runnable() { @Override public void run() { System.out.println(&quot;线程{&quot; + Thread.currentThread().getId() + &quot;}的初始值为:&quot; + threadLocal.get()); threadLocal.set(threadLocal.get() + 100); System.out.println(&quot;线程{&quot; + Thread.currentThread().getId() + &quot;}的累加值为:&quot; + threadLocal.get()); } }).start(); } } } 线程{10}的初始值为:0线程{12}的初始值为:2线程{11}的初始值为:1线程{11}的累加值为:101线程{12}的累加值为:102线程{10}的累加值为:100 由此可以看到，各个线程的threadlocal值是独立的。本线程对threadlocal中值的改动并没有影响到其他线程。 ThreadLocal的实现原理先查看ThreadLocal的get()方法源码： public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; } } return setInitialValue(); } 其中getMap的源码： ThreadLocalMap getMap(Thread t) { return t.threadLocals; } setInitialValue的源码： private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } createMap的源码： void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 查看以上源码后，我们可以了解get方法的流程： 1.首先获取当前线程 2.获取当前线程中的一个类型为ThreadLocalMap（这个类后面会讲到）的成员变量：threadLocals 3.如果threadLocalMap不为null，这通过当前ThreadLocal的引用作为key获取对应的value e。同时如果e不为null，返回e.value 4.如果threadLocalMap为null或者e为null，通过``setInitialValue``方法返回初始值。并且使用当前ThreadLocal的引用和value作为初始key与value创建一个新的threadLocalMap 总体设计思路：Thread维护了一个Map，key为ThreadLocal实例本身，value为真正需要存储的Object。 这样设计的好处：Map的Entry数量变小，性能提升。并且会随Thread一起销毁。 ThreadLocalMap解析先查看源码： /** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */ static class ThreadLocalMap { /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as &quot;stale entries&quot; in the code that follows. */ static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; } } } ThreadLocalMap是ThreadLocal的一个静态内部类。类上注释也解释了其基本实现方式： ThreadLocalMap是一个自定义的hash map，只适合用来维护现场局部变量。并且是包级私有。hash表中的key是一个ThreadLocal的弱引用。当没有对ThreadLocal的强引用，并且发送GC是，该Entry必然会被回收。 这里的弱引用也保证了不会因为线程迟迟没有结束，而ThreadLocal的强引用不存在了，保存在ThreadLocalMap中的Entry却还依然存在。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker学习系列五：构建oracle-jdk8镜像]]></title>
      <url>%2F2016%2F05%2F17%2Fdocker-five-oraclejdk8%2F</url>
      <content type="text"><![CDATA[前言由于docker官方的jdk镜像都是openjdk，与我们实际开发上线的jdk环境不符。因此需要构建一个oracle-jdk8的基础镜像。 参考的是dockerhub中一个oracle-jdk高star的配置：传送门 Dockerfile描述# AlpineLinux with a glibc-2.21 and Oracle Java 8 FROM alpine:3.3 MAINTAINER zacard &lt;mmaxiaolei@gmail.com&gt; # Java Version and other ENV ENV JAVA_VERSION_MAJOR=8 \ JAVA_VERSION_MINOR=92 \ JAVA_VERSION_BUILD=14 \ JAVA_PACKAGE=jdk \ JAVA_HOME=/opt/jdk \ PATH=${PATH}:/opt/jdk/bin \ LANG=C.UTF-8 # do all in one step RUN apk upgrade --update &amp;&amp; \ apk add --update curl ca-certificates bash &amp;&amp; \ for pkg in glibc-2.23-r1 glibc-bin-2.23-r1 glibc-i18n-2.23-r1; do curl -sSL https://github.com/andyshinn/alpine-pkg-glibc/releases/download/2.23-r1/${pkg}.apk -o /tmp/${pkg}.apk; done &amp;&amp; \ apk add --allow-untrusted /tmp/*.apk &amp;&amp; \ rm -v /tmp/*.apk &amp;&amp; \ ( /usr/glibc-compat/bin/localedef --force --inputfile POSIX --charmap UTF-8 C.UTF-8 || true ) &amp;&amp; \ echo &quot;export LANG=C.UTF-8&quot; &gt; /etc/profile.d/locale.sh &amp;&amp; \ /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib &amp;&amp; \ mkdir /opt &amp;&amp; curl -jksSLH &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; -o /tmp/java.tar.gz \ http://download.oracle.com/otn-pub/java/jdk/${JAVA_VERSION_MAJOR}u${JAVA_VERSION_MINOR}-b${JAVA_VERSION_BUILD}/${JAVA_PACKAGE}-${JAVA_VERSION_MAJOR}u${JAVA_VERSION_MINOR}-linux-x64.tar.gz &amp;&amp; \ gunzip /tmp/java.tar.gz &amp;&amp; \ tar -C /opt -xf /tmp/java.tar &amp;&amp; \ apk del curl glibc-i18n &amp;&amp; \ ln -s /opt/jdk1.${JAVA_VERSION_MAJOR}.0_${JAVA_VERSION_MINOR} /opt/jdk &amp;&amp; \ rm -rf /opt/jdk/*src.zip \ /opt/jdk/lib/missioncontrol \ /opt/jdk/lib/visualvm \ /opt/jdk/lib/*javafx* \ /opt/jdk/jre/plugin \ /opt/jdk/jre/bin/javaws \ /opt/jdk/jre/bin/jjs \ /opt/jdk/jre/bin/keytool \ /opt/jdk/jre/bin/orbd \ /opt/jdk/jre/bin/pack200 \ /opt/jdk/jre/bin/policytool \ /opt/jdk/jre/bin/rmid \ /opt/jdk/jre/bin/rmiregistry \ /opt/jdk/jre/bin/servertool \ /opt/jdk/jre/bin/tnameserv \ /opt/jdk/jre/bin/unpack200 \ /opt/jdk/jre/lib/javaws.jar \ /opt/jdk/jre/lib/deploy* \ /opt/jdk/jre/lib/desktop \ /opt/jdk/jre/lib/*javafx* \ /opt/jdk/jre/lib/*jfx* \ /opt/jdk/jre/lib/amd64/libdecora_sse.so \ /opt/jdk/jre/lib/amd64/libprism_*.so \ /opt/jdk/jre/lib/amd64/libfxplugins.so \ /opt/jdk/jre/lib/amd64/libglass.so \ /opt/jdk/jre/lib/amd64/libgstreamer-lite.so \ /opt/jdk/jre/lib/amd64/libjavafx*.so \ /opt/jdk/jre/lib/amd64/libjfx*.so \ /opt/jdk/jre/lib/ext/jfxrt.jar \ /opt/jdk/jre/lib/ext/nashorn.jar \ /opt/jdk/jre/lib/oblique-fonts \ /opt/jdk/jre/lib/plugin.jar \ /tmp/* /var/cache/apk/* &amp;&amp; \ echo &apos;hosts: files mdns4_minimal [NOTFOUND=return] dns mdns4&apos; &gt;&gt; /etc/nsswitch.conf # EOF 说明为什么要基于alpine:3.3看考官方java镜像中有关java:alpine的说明：传送门 摘录其中一段： This variant is highly recommended when final image size being as small as possible is desired 官方推荐使用基于alpine制作的镜像。 build镜像docker build -t oraclejdk8 . p.s:由于此镜像需要下载一些基础包和jdk，可能需要比较长的build时间，同时可能需要翻墙~ 测试镜像docker run -ti --rm oraclejdk8 java -version 保存镜像docker save -o jdk8.tar oraclejdk8 加载镜像docker -i jdk8.tar]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker学习系列四：Docker Compose]]></title>
      <url>%2F2016%2F05%2F13%2Fdocker-forth-docker-compose%2F</url>
      <content type="text"><![CDATA[什么是Docker ComposeDocker Compose是一个用来定义和运行docker应用服务（一个或者多个dacker容器应用）的工具。使用Compose file来配置管理docker应用服务。 为什么要用Docker Compose先来看下一下例子: docker run --name jetty -p 8881:8080 -p 22 -d jetty; docker run --name jenkins -p 8882:8080 --link -d jetty:jetty jenkins; 这段运行的命令是要先运行一个jetty容器，然后启用一个jenkins容器，同时链接到之前启动的jetty容器。命令显的冗长且极容易敲错或者漏敲。而且不容易记忆，迁移复杂。 如果使用Docker Compose的形式，只需配置一个docker-compose.yml文件即可。如下所示： services: jetty: images:jetty container_name:jetty ports: - &quot;8881:8080&quot; - &quot;22&quot; jenkins： imagesLjenkins container_name:jenkins ports: - &quot;8882:8080&quot; links: jetty 然后运行命令： docker-compose up -d 即可做到和之前的命令一样的效果。 如同Dockerfile之于镜像构建，Compose file使从Docker image运行Docker contain的过程结构化、透明化。并使之保留运行的记录。 Dokcer Compose安装安装使用如下命令： curl -L https://github.com/docker/compose/releases/download/1.7.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 同时推荐安装zsh的命令行补全： mkdir -p ~/.zsh/completion curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose version --short)/contrib/completion/zsh/_docker-compose &gt; ~/.zsh/completion/_docker-compose 测试是否安装成功： docker-compose --version Compose file语法具体请参考：官方说明 在此只摘录几个常用的。 buildbuild配置项有2种形式。String形式表示镜像build所需的DOckerfile路径；对象形式表示镜像build所需的的dockerfile和参数。例如以下命令： buile: ./dir build: context: ./dir dockerfile: Dockerfile-redis args: buildno: 1 container_name指定一个自定义的容器名称来代替默认生成的名称。 environment增加一个环境变量。有2种形式。数组形式和字典形式。例如以下所示： environment: RACK_ENV: development SHOW: &apos;true&apos; SESSION_SECRET: environment: - RACK_ENV=development - SHOW=true - SESSION_SECRET image指定容器启动的镜像。可以是镜像名称或者是镜像id。例如以下所示： image: redis image: ubuntu:14.04 image: tutum/influxdb image: example-registry.com:4000/mysql image: a4bc65fd 如果镜像不存在，会自动pull下来。 links链接容器和其他服务。可以指定服务名称和服务的别名。例如以下所示： web: links: - db - db:mysql - redis ports开发端口。例如以下所示： ports: - &quot;3000&quot; - &quot;3000-3005&quot; - &quot;8000:8000&quot; - &quot;9090-9091:8080-8081&quot; - &quot;49100:22&quot; - &quot;127.0.0.1:8001:8001&quot; - &quot;127.0.0.1:5000-5010:5000-5010&quot; volumes挂载主机路径到容器内。例如以下所示： volumes: - /var/lib/mysql - /opt/data:/var/lib/mysql 根据Compose file启动服务docker-compose up docker-compose up -d 具体更多命令，请 docker-compose --help docker-compose [command] --help]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker学习系列三：Registry]]></title>
      <url>%2F2016%2F05%2F13%2Fdocker-third-registry%2F</url>
      <content type="text"><![CDATA[什么是registryDocker Registry是一个docker仓库，用来存储和分享docker镜像。类似于Github。 为什么需要docker registry这里应该说为什么需要配置一个私有的docker registry。我们知道，已经有dockerhub了，而且官方也提供私有仓库的功能。 因为访问官方的dockerhub，是要良好的网络。而且当我们部署发布的时候，肯定不希望dockeruhb在维护或者故障。而我们配置私有的docker registry可以很好的避免此类问题。 部署私有的docker registrypull官方的registrydocker pull registry:2.4 运行registry命令： docker run -d -p 5000:5000 --restart=always --name registry -v /data/registry:/var/lib/registry registry:2.4 使用Compose file编排(创建docker-compose.yml)： registry: restart: always image: registry:2.4 container_name: registry ports: - 5000:5000 volumes: - /data/registry:/var/lib/registry 然后使用命令： docker-compose up -d 配置授权用户的registry删除启动的registry:docker stop registry&amp;docker rm registry 或者： docker-compose stop registry&amp;docker-compose rm registry 创建授权用户目录并且创建一个授权用户mkdir auth docker run --rm --entrypoint htpasswd registry：2.4 -Bbn testuser testpasswd &gt; auth/htpasswd 以授权用户的方式启动reigstry命令： docker run -d -p 5000:5000 --restart=always --name registry -v /data/registry:/var/lib/registry -v `pwd`/auth:/auth -e &quot;REGISTRY_AUTH=htpasswd&quot; -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; -e &quot;REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd&quot; registry:2.4 Compose file编排方式： registry: restart: always image: registry:2.4 container_name: registry ports: - 5000:5000 volumes: - /data/registry:/var/lib/registry - /data/auth:/auth environment: REGISTRY_AUTH: htpasswd REGISTRY_AUTH_HTPASSWD_REALM: Registry Realm REGISTRY_AUTH_HTPASSWD_PATH: /auth/htpasswd 然后使用命令： docker-compose up -d 查看log是否启动成功： docker logs registry 登录到私服： docker login localhost:5000 输入用户名、密码等信息。然后上传镜像测试： docker pull busybox docker tab busybox localhost:5000/busybox docker push localhost:5000/busybox 使用证书（CA）认证的域名方式启动私服参考：官方说明 如要使用自签名证书方式启动，参考：官方说明]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker学习系列二：Dockerfile]]></title>
      <url>%2F2016%2F05%2F11%2Fdocker-second-dockerfile%2F</url>
      <content type="text"><![CDATA[前言构建或者修改Docker image建议都使用Dockerfile。因为通过Dockerfile构建image或者修改image是透明的、可记录的，且无需重复的镜像和容器的创建过程。甚至可以使用版本控制（如：git）来保存Dockerfile的修改记录，以保留docker image完整的生命周期。 参考：官方Dockerfile说明 Dockerfile基本语法Dockerfile支持的语法命令如下： INSTRUCTION argument 指令不区分大小写。但是，命名约定全部为大写。 所有Dockerfile都必须以FROM命令开始。FROM命令会指定基于哪个基础镜像创建，接下来的命令也会基于这个基础镜像。FROM命令可以多次使用，表示会创建多个镜像。集体突发如下： FROM &lt;IMAGE NAME&gt; 例如： FROM ubuntu 上面的指定告诉我们，新的镜像将基于Ubuntu的镜像来构建。 继FROM命令，Dockerfile还提供了一些其他的命令以实现自动化。这些命令的顺序就是他们的执行顺序。 MAINTAINERMAINTAINER命令用来设置改镜像的作者。语法如下： MAINTAINER &lt;author name&gt; 例如： MAINTAINER zacard RUNRUN有2种形式，shell和exec。shell形式的语法如下： RUN &lt;command&gt; exec形式的语法如下： RUN [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] ADDADD复制文件的命令。它有2个参数和。source可以是url或者是启动配置上下文中的一个文件。destination是容器内的路径。语法如下： ADD &lt;source&gt; &lt;destination&gt; CMDCMD提供了容器默认的执行命令。Dockerfile只允许使用一次CMD命令。使用多个CMD会抵消之前所有的命令，只有最后一个命令生效。CMD命令有3种形式： // exec形式，推荐的使用形式 CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] // 默认ENTRYPOINT的param形式 CMD [&quot;param1&quot;,&quot;param2&quot;] // shell形式 CMD command param1 param2 ENTRYPOINTENTRYPOINT命令配置给容器一个可执行的命令，使镜像创建容器时默认运行一个可执行文件。且和CMD类似，Dockerfile也只允许一个ENTRYPOINT命令，多个ENTRYPOINT只会执行最后一个命令。语法如下： // exec形式，推荐的使用形式 ENTRYPOINT [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] // shell形式 ENTRYPOINT command param1 param2 CMD和ENTRYPOINT如何相互影响2个命令相互间的影响如下表格所示： No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”,”p1_entry”] No CMD error,not allowrd /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”,”p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_cmd p1_cmd exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”,”p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry p1_cmd p2_cmd exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd 另外，docker run命令中的参数会传递给ENTRYPOINT命令，而不用担心被覆盖（CMD与之相反）。所以ENTRYPOINT与CMD结合使用效果会更好。 EXPOSEEXPOSE命令指定容器在运行时监听的端口。语法如下： EXPOSE &lt;port&gt; [&lt;port&gt;...] 例如： // 映射容器私有端口80到共有端口8080 EXPOSE 80:8080 // 开放私有端口80 EXPOSE 80 注意，永远不要使用Dockerfile映射公有端口。不然你将可能只能运行一个容器化应用程序的实例。 WORKDIRWORKDIR命令指定了RUN、CDM和ENTRYPOINT命令的工作目录。语法如下： WORKDIR /path/to/workdir ENVENV命令设置环境变量。使用键值对的方式，增加了运行程序的灵活性。语法如下： // 推荐 ENV &lt;key&gt;=&lt;value&gt; ENV &lt;key&gt; &lt;value&gt; USER给镜像运行时设置一个UID。语法如下： USER &lt;uid&gt; VOLUME授权访问从容器内到宿主主机上的目录。语法如下： VOLUME [&quot;/data&quot;] Dockerfile的一些建议 不要开机初始化 不要在构建中升级版本 使用小型基础镜像，建议FROM alpine:3.3 尽量使用格式一致的Dockerfile，这样能使用缓存 不要在构建中升级版本，如在容器中apt-get upgrade 使用特点的标签。如FROM debian:jeesie,而不是FROM debian 常见的命令组合。如：apt-get update与atp-get install组合。此外使用\格式化成多行命令。这样能够最大程度的应该缓存。 使用自己的基础镜像]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker学习系列一：为何要用Docker]]></title>
      <url>%2F2016%2F05%2F11%2Fdocker-one-why%2F</url>
      <content type="text"><![CDATA[前言强烈建议先看下官方的文档：传送门 同时建议看下Flux7的Dock入门教程：传送门 本系列需要了解Docker的一些基本命令为前提。 什么是DockerDocker是一个开源的应用容器引擎。基于Namspaces、Control Groups和UnionFS，保证容器了的轻巧、隔离和易移植。 为何要用Docker build once, configure once and run anywhere 速度飞快 优雅的隔离架构 cpu/内存消耗低 快速启动/关闭/销毁 开源 种种优点契合了目前主流的一些需求： 简化了大规模的集群部署步骤 方便的持续集成和持续部署 微服务架构 一次性的/定时的任务 快速部署 一致的开发测试环境 演示、使用环境 解决设备成本，充分利用资源 技术方案快速验证 更多…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JSqlParser使用示例]]></title>
      <url>%2F2016%2F05%2F10%2FJSqlParser-examples%2F</url>
      <content type="text"><![CDATA[背景由于业务需要，需要编写一个mybatis插件来统一处理一类sql，并且需要对sql动态处理。然而解析sql成为一件繁琐的工作，果断github上查找是否有sql解析的项目。一个300多star的sql解析构建项目：JSqlParser。 由于JSqlParser的github主页的使用说明略简单，查看测试类，也没有覆盖常用的使用需求。于是，这里整理了一些个人实际使用的一些方法。 使用示例查询返回增加一列代码如下： /** * 测试查询返回增加一列 */ @Test public void testAddSelectColumn() throws Exception { Select select = (Select) CCJSqlParserUtil.parse(&quot;select name from user where id = 1&quot;); SelectUtils.addExpression(select, new Column(&quot;mail&quot;)); Assert.assertEquals(select.toString(), &quot;SELECT name, mail FROM user WHERE id = 1&quot;); } 查询语句增加where条件代码如下： /** * 测试查询语句增加where条件 */ @Test public void testAddWhereCondition() throws Exception { Select select = (Select) CCJSqlParserUtil.parse(&quot;select name from user&quot;); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); if (plainSelect.getWhere() == null) { EqualsTo equalsTo = new EqualsTo(); equalsTo.setLeftExpression(new Column(&quot;id&quot;)); equalsTo.setRightExpression(new LongValue(1000L)); plainSelect.setWhere(equalsTo); } Assert.assertEquals(select.toString(), &quot;SELECT name FROM user WHERE id = 1000&quot;); } 增加where查询条件代码如下： /** * 测试增加where查询条件 */ @Test public void testAddCondition() throws Exception { Select select = (Select) CCJSqlParserUtil.parse(&quot;select name from user where id = 1000&quot;); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); // 原where表达式 Expression where = plainSelect.getWhere(); // 新增的查询条件表达式 EqualsTo equalsTo = new EqualsTo(); equalsTo.setLeftExpression(new Column(&quot;name&quot;)); equalsTo.setRightExpression(new StringValue(&quot;&apos;张三&apos;&quot;)); // 用and链接条件 AndExpression and = new AndExpression(where, equalsTo); // 设置新的where条件 plainSelect.setWhere(and); Assert.assertEquals(select.toString(), &quot;SELECT name FROM user WHERE id = 1000 AND name = &apos;张三&apos;&quot;); } 增加null查询条件代码如下： /** * 测试null条件 */ @Test public void testNullCondition() throws Exception { Select select = (Select) CCJSqlParserUtil.parse(&quot;select name from user where id = 1000&quot;); PlainSelect plainSelect = (PlainSelect) select.getSelectBody(); // 原where表达式 Expression where = plainSelect.getWhere(); // 新增的null判断条件 IsNullExpression isNullExpression = new IsNullExpression(); isNullExpression.setLeftExpression(new Column(&quot;name&quot;)); isNullExpression.setNot(true); // 用and链接条件 AndExpression and = new AndExpression(where, isNullExpression); // 设置新的where条件 plainSelect.setWhere(and); Assert.assertEquals(select.toString(), &quot;SELECT name FROM user WHERE id = 1000 AND name IS NOT NULL&quot;); } 总结JSqlParser的代码结构和使用逻辑总体上算上简单易懂，基本看下项目的包结构以及类注释就能明白大致的用法。其中不乏一些join、group等高阶sql操作。如有sql解析，动态处理方面的需求，JSqlParser还是一个很好的选择。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[testng期望异常和期望异常信息的使用]]></title>
      <url>%2F2016%2F04%2F22%2Ftestng-exceptexception-message%2F</url>
      <content type="text"><![CDATA[背景当我们编写单元测试的时候，经常会使用到expectedExceptions来测试程序的报错是否达到预期。但是，一般的系统设计中，异常都是统一处理成一个自定义的异常，用不同的错误code和错误message来区分不同的错误信息。这就需要用到testng的expectedExceptionsMessageRegExp来匹配抛出的异常message是否是预期的。 使用代码如下： public class MyTest { @Test(expectedExceptions = MyException.class, expectedExceptionsMessageRegExp = &quot;.*error-code-1001.*&quot;) public void testcaseOne() { System.out.println(&quot;test expectedExceptionsMessageRegExp.&quot;); throw new MyException(“error-code-1001”，“系统错误”); } } 这个测试方法会被判定通过。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[idea设置google code style]]></title>
      <url>%2F2016%2F04%2F11%2Fidea-google-code-style%2F</url>
      <content type="text"><![CDATA[背景idea支持自定义的code style，并且google code style也提供了正对idea的xml配置，直接导入就可以在idea中使用google提倡的code style了。 操作 从github上clone google code style 复制对应的xml配置（如intellij-java-google-style.xml）到“~/Library/Preferences/IDEA/codestyles/”下 重启idea在Prefrence-&gt;Editor—&gt;Code Stytle-&gt;Java,选择GoogleStyle即可]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ideaVim使用记录(持续更新)]]></title>
      <url>%2F2016%2F03%2F23%2FideaVim-use-record%2F</url>
      <content type="text"><![CDATA[背景一直想脱离触摸板（鼠标已经脱离），纯键盘开发。vim肯定是首选，于是idea上的vim插件ideaVim进入熟悉阶段。由于vim的操作也不熟悉，所以在此记录本人开发中用到的一些快捷键操作。 keymap 记录 跳转到指定行：{行数}g 标签特切换：gt或者gT,前者顺序切换，后者逆向切换 单词移动：w/W，移动到下个单词开头；b/B,倒退到上个单词开头。大写的会忽略标点。命令前加数字表示执行次数，如2W 删除当前单词并进入插入模式：cw 撤销：u;恢复被撤销的操作：ctrl+r 复制单词，替换复制内容：yiw-&gt;viwp]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用java8将list转为map]]></title>
      <url>%2F2016%2F03%2F17%2Fjava8-list-to-map%2F</url>
      <content type="text"><![CDATA[常用方式代码如下： public Map&lt;Long, String&gt; getIdNameMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, Account::getUsername)); } 收集成实体本身map代码如下： public Map&lt;Long, Account&gt; getIdAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, account -&gt; account)); } account -&gt; account是一个返回本身的lambda表达式，其实还可以使用Function接口中的一个默认方法代替，使整个方法更简洁优雅： public Map&lt;Long, Account&gt; getIdAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, Function.identity())); } 重复key的情况代码如下： public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity())); } 这个方法可能报错（java.lang.IllegalStateException: Duplicate key），因为name是有可能重复的。toMap有个重载方法，可以传入一个合并的函数来解决key冲突问题： public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity(), (key1, key2) -&gt; key2)); } 这里只是简单的使用后者覆盖前者来解决key重复问题。 指定具体收集的maptoMap还有另一个重载方法，可以指定一个Map的具体实现，来收集数据： public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity(), (key1, key2) -&gt; key2, LinkedHashMap::new)); }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[http压力测试小工具]]></title>
      <url>%2F2016%2F03%2F15%2Fstress-test-tool%2F</url>
      <content type="text"><![CDATA[背景最近在寻找一款小而美的http压力测试工具，以方便本机快速测试。果断github里搜索。不负众望啊，一款4000多star的http压力测试小工具。果断试用。（ps：感谢开源，拥抱开源） 安装工具叫做boom(传送门)。 go get github.com/rakyll/boom 这里要注意，这个工具是用go语言写的，所以你要先有go语言环境，并且是要配置GOPATH。 使用boom -n 100 -c 10 http://www.baidu.com -n：请求连接数-c：请求并发数其他参数参考官方github。 ps：boom命令在${GOPATH}/bin/下]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于java的一些冷知识]]></title>
      <url>%2F2016%2F02%2F29%2Fjava-cold-one%2F</url>
      <content type="text"><![CDATA[背景最近开始重新翻阅了《Think in Java》这本书。果然，温故而知新，发现了一些冷知识。 java中的gotogoto是java中的保留字，但是却不是java中的关键字，你并不能在代码中使用goto。但是java能够使用break/continue和标签达到goto的效果。然而，需要注意的是，java里唯一需要用到这种效果的理由就是因为循环嵌套的存在，并且想从循环嵌套中break/continue。 到目前为止，本人阅读过的开源源码中且有记忆的，有使用过类似这种goto效果的，只有Google的Gson。 以返回值区分重载方法在我们学习重载的时候就知道，只能以方法和方法的形参列表作为标准。那为何不能以方法的返回值来区分呢？ 如以下2个方法： void f(){} int void f(){return 1;} 编译器可以通过int x=f()这里的语义来区分需要调用的方法。但是，有时候你并不关心方法的返回值，你想要的是方法调用的其他效果（这通常称为“为了副作用而调用”）。所以，像f()这种调用方法，编译器就无法区分了。 this关键字为何我们能在一个类的方法（非静态方法）中使用this关键字，即当前对象呢？是因为在调用该实例对象的方法的时候，编译器“偷偷”的帮我们把当前实例对象的传进来了。 逗号操作符这里说的不是逗号分隔符，逗号用作分隔符时用来函数的不同参数。java中唯一用到逗号操作符的地方就是for循环的控制表达式。在控制表达式的初始化和步进控制部分，可以使用一系列由逗号分隔的语句，而且那些语句俊辉独立执行。例如一下代码： for(int i = 1, j = i + 10; i&lt;5; i++, j = i * 2){ } else if在java中else if不是关键字。由于java是自由格式语言。else if其实相当于： if(a==1){ }else{ if(a==2){ } } 大家都知道，if和else的大括号是可以省略的。于是就成了else if。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于mybatis3.3.1批量插入回写id的实践]]></title>
      <url>%2F2016%2F02%2F18%2Fmybatis3-multiple-rows-write-bace-id%2F</url>
      <content type="text"><![CDATA[背景今日，注意到mybatis3.3.1正式发布，果断查看了更新内容（传送门）。大致浏览了下，其中有一项喜人的改进： Support insert multiple rows and write-back id 批量插入支持id回写了！我们知道，以往如果批量插入，需要获取插入后的ids，是需要根据特定条件反查的。但是，有了这个特性了，完全省去了这一多余查询过程。 实践迫不及待的试了一把。代码如下： pom依赖： &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.2.4&lt;/version&gt; &lt;/dependency&gt; mapper interface: public interface AccountDAO { /** * 批量插入账户 * * @param accounts 账户集合 * @return 成功插入数量 */ int batchInsert(List&lt;Account&gt; accounts); } mapper xml: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt; &lt;mapper namespace=&quot;com.xkeshi.shop.dao.AccountDAO&quot;&gt; &lt;insert id=&quot;batchInsert&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt; INSERT INTO account (username, password) VALUES &lt;foreach collection=&quot;accounts&quot; item=&quot;account&quot; index=&quot;index&quot; separator=&quot;,&quot; &gt; (#{account.username},#{account.password}) &lt;/foreach&gt; &lt;/insert&gt; &lt;/mapper&gt; 测试类： @ContextConfiguration(locations = {&quot;classpath*:spring-test.xml&quot; }) @Transactional public class AccountDAOTest extends AbstractTestNGSpringContextTests { @Autowired private AccountDAO accountDAO; public void testBatchInsert() { List&lt;Account&gt; accounts = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 3; i++) { Account account = new Account(); account.setUsername(&quot;测试&quot; + i); account.setPassword(&quot;888&quot;); accounts.add(account); } int result = accountDAO.batchInsert(accounts); Assert.assertEquals(3, result); for (Account account : accounts) { Assert.assertNotNull(account.getId()); } } } 然而，测试类结果直接报错： org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.executor.ExecutorException: Error getting generated key or setting result to parameter object. Cause: org.apache.ibatis.binding.BindingException: Parameter &apos;id&apos; not found. Available parameters are [accounts, param1] 跟踪这项修改的提交记录（传送门）发现src/main/java/org/apache/ibatis/executor/keygen/Jdbc3KeyGenerator.java的修改记录： private Collection&lt;Object&gt; getParameters(Object parameter) { Collection&lt;Object&gt; parameters = null; if (parameter instanceof Collection) { parameters = (Collection) parameter; } else if (parameter instanceof Map) { Map parameterMap = (Map) parameter; // when you insert a List(or Array). // If you want retrieve the auto-increment value or default value. // You should not to use @Param annotations, You must use a single parameter(List or Array). if (parameterMap.containsKey(&quot;collection&quot;)) { parameters = (Collection) parameterMap.get(&quot;collection&quot;); } else if (parameterMap.containsKey(&quot;list&quot;)) { parameters = (List) parameterMap.get(&quot;list&quot;); } else if (parameterMap.containsKey(&quot;array&quot;)) { parameters = Arrays.asList((Object[]) parameterMap.get(&quot;array&quot;)); } } if (parameters == null) { parameters = new ArrayList&lt;Object&gt;(); parameters.add(parameter); } return parameters; } 也就是说入参的集合名称必须叫“collection”、“list”、“array”才会生效哦！ 修改代码，mapper interface： public interface AccountDAO { /** * 批量插入账户 * * @param accounts 账户集合 * @return 成功插入数量 */ int batchInsert(List&lt;Account&gt; list); } mapper xml: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt; &lt;mapper namespace=&quot;com.xkeshi.shop.dao.AccountDAO&quot;&gt; &lt;insert id=&quot;batchInsert&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt; INSERT INTO account (username, password) VALUES &lt;foreach collection=&quot;list&quot; item=&quot;account&quot; index=&quot;index&quot; separator=&quot;,&quot; &gt; (#{account.username},#{account.password}) &lt;/foreach&gt; &lt;/insert&gt; &lt;/mapper&gt; 重新运行测试类，成功！ 思考mybatis这项修改的原理就是，当集合或者数组的parameter名称为“collection”、“list”、“array”，同时设置了useGeneratedKeys=”true” ，这回把生成的id值回写到对应集合或者数组中的实体中。 但是通过parameter名称区别是否回写总是不太优雅，个人感觉应该新增一个注解，添加在相应的集合或者数组上，标示这是一个批量插入的实体，是需要回写id到这里的。 总而言之，这是一个方便大家的改动，enjoy吧~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac osx推荐2款键盘党软件]]></title>
      <url>%2F2016%2F02%2F03%2Fmac-kill-mouse%2F</url>
      <content type="text"><![CDATA[背景由于平时键盘操作大于触摸板（无鼠标操作），然后前段时间整了个机械键盘，连触摸板都不想用了。于是去网上找了几款键盘党神器级软件，基本能解放触摸板了~ shortcat Shortcat lets you keep your hands on the keyboard and boost your productivity! Shortcat is a keyboard tool for OS X that lets you “click” buttons and control your apps with a few keystrokes. Think of it as Spotlight for the user interface Shortcat 是一款OS X上的键盘工具，能够让你的双手在键盘上操作代替鼠标或者触摸板的操作，提高生产力。可以把他想象成用户界面上的Spotlight 效果如下: karabiner A powerful and stable keyboard customizer for OS X karabiner是OS X上的一个功能强大的键盘映射工具 个人主要用来修改机械键盘中的一些pc专用的按键和一些Right按键 具体修改方法参考：传送门键盘对应的keycode参考：传送门 后感使用这2款工具后，95%的操作是键盘完成的。非常方便实用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用java8编写整洁的逻辑判断]]></title>
      <url>%2F2016%2F02%2F01%2Fjava8-new-one%2F</url>
      <content type="text"><![CDATA[背景 IN-LINE PREDICATES CAN CREATE A MAINTENANCE NIGHTMARE 代码内部的逻辑判断将会成为代码可维护性的恶梦。 使用lambda表达式和steam接口对集合进行常见的操作是非常畅快的。例如如下代码： public List&lt;Person&gt; getAdultMales (List&lt;Person&gt; persons) { return persons.stream().filter(p -&gt; p.getAge() &gt; ADULT &amp;&amp; p.getSex() == SexEnum.MALE ).collect(Collectors.&lt;Person&gt;toList()); } 这很简便！但是这么写却会导致高昂的维护成本。在一个企业级的应用程序中，您的开发团队肯定会有编写重复的业务逻辑判断的代码，这肯定不是你想要看到的项目。因为对于一个易维护，可扩展的企业应用来说，它违反了以下3个重要的原则： DRY(don`t repeat yourself，不要重复代码)：重复一次的代码对于一个“懒惰”的开发者来说就是不好的；同时这也使得您的代码难以维护，因为当业务逻辑改变时，你很难保持业务规则的一致性。 Readability（可读性）：根据整洁代码的最佳实践，您编写的80%的代码应该是调用已经存在的代码。相对于简单的一行调用方法来说，lambda表达式仍然是有点难以阅读。 Testability（易测性）：您的业务代码需要良好的测试。并且非常建议对复杂的逻辑判断做单元测试。而当你把逻辑判断那部分分离出来后，测试将会异常的简单。 从我个人的角度看，这样的方法仍然包含太多的模板代码。 优化幸运的是，在单元测试的世界中，有一个非常好的建议能够改变这一问题。 请看如下代码: import static com.xkeshi.shop.imp.PersonPredicate.*; /** * Predicate测试类 * * @author zacard * @since 2016-02-01 16:11 */ public class PredicateTest { public List&lt;Person&gt; getAdultMales(List&lt;Person&gt; persons) { return persons.stream().filter(isAdultMale()).collect(Collectors.&lt;Person&gt;toList()); } } 我们做了什么： 创建了一个PersonPredicate类 定义了一个lambda形式的断言静态工厂方法 静态导入这个工厂方法 PersonPredicate类的定义如下： /** * @author zacard * @since 2016-02-01 17:41 */ public class PersonPredicate { public static final int ADULT = 18; public static Predicate&lt;Person&gt; isAdultMale() { return p -&gt; p.getAge() &gt; ADULT &amp;&amp; p.getSex() == SexEnum.MALE; } } 等等，我们为什么不直接在Person类中创建一个叫做“isAdultMale”的boolean函数？这确实是一种选择。。。但是随着时间的推移，项目变的庞大，并且加载越来越多的功能和数据是，你仍然会打破代码整洁的原则： 类的功能和条件变得臃肿 类和单元测试变得巨大，难以处理和修改 添加默认方法根据面向对象的思想，我们可以想象，一些操作会经常的在对象上被执行（比如过滤操作）。考虑到这点，我们可以让对象服务实现一个接口来定义一个对象的行为，并提供一些默认的方法（java8中允许接口有默认方法的实现），是很有意义的。 例如以下代码： public interface PersonOperations { default List&lt;Person&gt; filter(List&lt;Person&gt; persons, Predicate&lt;Person&gt; predicate) { return persons.stream().filter(predicate).collect(Collectors.toList()); } } 当我们的person服务实现了这个接口，我们可以让代码更简洁： import java.util.List; import static com.xkeshi.shop.imp.predicate.PersonPredicate.isAdultMale; public class PersonService implements PersonOperations { public List&lt;Person&gt; getAdultMales(List&lt;Person&gt; persons) { return filter(persons, isAdultMale()); } } 结论从长远发展来看，将业务逻辑判断移到一个逻辑判断的辅助类中，将提供以下几个优点： 你的逻辑判断很容易修改和测试 你的对象服务类将保持整洁，让你集中在业务流程而不是业务逻辑判断 提高了代码的重用性，减少了代码的维护成本 进一步分离出了业务的操作关系 以上从这里（传送门）翻译整理。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[dubbo service单元测试参数校验的问题]]></title>
      <url>%2F2016%2F01%2F29%2Fdubbo-bean-validation%2F</url>
      <content type="text"><![CDATA[背景当我们编写dubbo service端的单元测试的时候，并且设置了dubbo的validation是在客户端（服务消费端）校验的话，那么测试类中的基于注解的参数校验将不会生效。 解决办法最简单的办法就是直接编写一个aop类，在方法调用前先做参数校验，代码如下： /** * 参数校验aop切面 * 在dubbo service test类中,使参数校验生效 * * @author zacard * @since 2016-01-29 13:56 */ @Component @Aspect public class ValidatorAspect { private static final Logger logger = LoggerFactory.getLogger(ValidatorAspect.class); /** * 参数校验类 */ private static Validator validator; /** * 内部类连接符 */ public static final String INNERCLASSSTR = &quot;$&quot;; // 定义切入点:dubbo service @Pointcut(value = &quot;execution(public * com.zacard.*.apis.*.*(..))&quot;) private void dubboServicePointcut() { } // 前置aop @Before(value = &quot;dubboServicePointcut()&quot;) public void beforeAdvice(JoinPoint pj) throws Throwable { // 1.校验方法入参 validateMethodParams(pj); // 2.校验入参内的属性 validateParamsProperty(pj); } /** * 校验方法入参 * * @param pj aop切面入参 */ private void validateMethodParams(JoinPoint pj) throws Exception { MethodSignature signature = (MethodSignature) pj.getSignature(); // 代理的方法 Method method = signature.getMethod(); // 是否需要校验方法入参 if (isNeedValidateMethod(method)) { // 代理类 Class aopClass = pj.getTarget().getClass(); // 方法参数列表 Object[] parms = pj.getArgs(); Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations = thegetValidateResultForMethod(aopClass.newInstance(), method, parms); if (!CollectionUtils.isEmpty(violations)) { throw new ValidateException(&quot;TEST_ERROR_001&quot;, &quot;参数校验未通过=&gt;{&quot; + getErrorMsg(violations) + &quot;}&quot;); } } } /** * 校验入参属性 * * @param pj aop切面入参 */ private void validateParamsProperty(JoinPoint pj) { // 方法参数列表 Object[] params = pj.getArgs(); if (params == null || params.length &lt; 1) { return; } // 分组校验注解类 Class methodAnnotation = getMethodGroupAnnotation(pj.getTarget().getClass(), pj.getSignature().getName()); for (Object param : params) { if (param == null) { continue; } Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations = getValidateResultForParam(param, methodAnnotation); if (!CollectionUtils.isEmpty(violations)) { throw new ValidateException(&quot;TEST_ERROR_001&quot;, &quot;参数校验未通过=&gt;{&quot; + getErrorMsg(violations) + &quot;}&quot;); } } } /** * 判断是否需要先验证方法中的入参 * * @param method aop方法类 */ private boolean isNeedValidateMethod(Method method) { for (Annotation[] annotations : method.getParameterAnnotations()) { if (annotations.length &gt; 0) { return true; } } return false; } /** * 获取aop方法对应的分组校验注解类 * * @param aopClass aop类 * @param methodName aop方法名称 * @return 分组校验注解类 */ private Class getMethodGroupAnnotation(Class aopClass, String methodName) { Class[] interfaces = aopClass.getInterfaces(); if (interfaces.length &gt; 0) { String interfaceName = interfaces[0].getName(); if (logger.isInfoEnabled()) { logger.info(&quot;aop method=&gt;{&quot; + aopClass.getName() + &quot;.&quot; + methodName + &quot;()}&quot;); } // 判断是否有对应方法的分组校验注解 try { String annotationName = interfaceName + INNERCLASSSTR + getDubboTypeAnnonName(methodName); return Class.forName(annotationName); } catch (ClassNotFoundException e) { // 不存在这个注解 } } return null; } /** * 获取dubbo验证规则下的分组校验的注解名称 * * @param methodName aop的方法名称 * @return 注解名称 */ private String getDubboTypeAnnonName(String methodName) { return methodName.substring(0, 1).toUpperCase() + methodName.substring(1); } /** * 校验方法中的入参,并返回验证结果 * * @param classInstance 方法所在类的实例 * @param method 方法类 * @param params 方法值数组 * @return 校验结果 */ private Set&lt;ConstraintViolation&lt;Object&gt;&gt; getValidateResultForMethod(Object classInstance, Method method, Object[] params) { return getValidator().forExecutables().validateParameters(classInstance, method, params); } /** * 校验入参 * * @param param 入参 * @param methodAnnotation 分组注解类 * @return 校验结果 */ private Set&lt;ConstraintViolation&lt;Object&gt;&gt; getValidateResultForParam(Object param, Class methodAnnotation) { if (methodAnnotation == null) { return getValidator().validate(param); } return getValidator().validate(param, methodAnnotation); } /** * 从校验结果中格式化出错误信息 * * @param violations 参数校验结果 * @return 错误信息集合, 格式:[a:reason] */ private List&lt;String&gt; getErrorMsg(Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations) { List&lt;String&gt; result = new ArrayList&lt;&gt;(); if (violations == null || violations.isEmpty()) { return result; } return violations.stream() .map(violation -&gt; violation.getPropertyPath() + &quot;:&quot; + violation.getMessage()) .collect(Collectors.toList()); } /** * 获取validator * * @return 参数校验类 */ private Validator getValidator() { if (validator == null) { ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); validator = factory.getValidator(); } return validator; } xml需要开启aop注解支持： &lt;!--开启aop注解支持--&gt; &lt;aop:aspectj-autoproxy /&gt; 同时需要扫描ValidatorAspect类所在的包，例如： &lt;context:component-scan base-package=&quot;com.zacard.core.test&quot; /&gt; 依赖的jar包,pom.xml配置： &lt;!--AspectJ--&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;1.6.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;aopalliance&lt;/groupId&gt; &lt;artifactId&gt;aopalliance&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 需要注意的一个地方如果在单元测试类中，同时使用了Mock一类的包(例如：mockito)，可能会使Mock失效。 失效原因由于测试类是被aop代理的类,使用mock注入的一些bean或者属性会注入到代理类中,所以会失败. 解决办法编写一个工具类，将mock对象注入到真实的类中，代码如下： /** * Mock注入工具类 * * @author zacard * @since 2016-01-29 14:26 */ public class MockWithAopUtils { private static final Logger logger = LoggerFactory.getLogger(MockWithAopUtils.class); /** * 注入mock对象到被aop代理的原bean中 * * @param target 真实的bean * @param propertyName 被mock属性名称 * @param mock mock对象 */ public static void setMocks(Object target, String propertyName, Object mock) { // 1.获取被aop代理的原始bean Object realBean = target; try { realBean = unwrapProxy(target); } catch (Exception e) { logger.error(&quot;获取被aop代理的原始bean失败!&quot;, e); } // 2.注入mock对象 ReflectionTestUtils.setField(realBean, propertyName, mock); } /** * 获取真实的被代理类 * * @param bean 代理类 * @return 真实bean * @throws Exception */ private static Object unwrapProxy(Object bean) throws Exception { if (AopUtils.isAopProxy(bean) &amp;&amp; bean instanceof Advised) { Advised advised = (Advised) bean; bean = advised.getTargetSource().getTarget(); } return bean; } }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu中添加zsh(oh-my-zsh)的环境变量]]></title>
      <url>%2F2016%2F01%2F23%2Flinux-zsh-env%2F</url>
      <content type="text"><![CDATA[背景听说有个shell叫zsh和oh-my-zsh完爆Ubuntu默认的bash，果断安装试用。 命令： apt-get install -y zsh wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh 安装完成后体验了下被网友力推的几个功能和插件（详情可以见：传送门），确实方便好用。 但是碰到个问题：无法读取到配置在~/.bashrc中的环境变量，这也是摆明的问题，你shell都换了，zsh这么可能读到bash的配置文件中的环境变量。 解决方法只需要复制~/.basrc中的环境变量的配置到~/.zshrc中，然后 source ~/.zshrc 即可生效。 大家快去体验zsh(oh-my-zsh)带来的便利吧~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[IntelliJ IDEA内存优化]]></title>
      <url>%2F2016%2F01%2F21%2Fidea-vmoptions-setting%2F</url>
      <content type="text"><![CDATA[背景经常有人抱怨说idea反应慢，吃内存。于是google到了一篇老外对idea内存配置优化方案的比较和测试，传送门 引用 Don’t be a Scrooge and give your IDE some more memory 不要做守财奴，给IDE多分配点内存吧。 哈哈！老外已经说了，idea慢的原因，基本是内存给的不够。 结论这里直接拿老外的测试结论来提供一份idea.vmoptions的配置： -Xms2g -Xmx2g -XX:ReservedCodeCacheSize=1024m -XX:+UseCompressedOops 就是这样了。。。 至于idea.vmoptions怎么用，请看传送门 have fun~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于使用maven jetty插件启动慢的解决方法]]></title>
      <url>%2F2016%2F01%2F19%2Fmaven-jetty-slow%2F</url>
      <content type="text"><![CDATA[背景使用maven的jetty插件启动web（spring）项目时，可能会遇到项目启动很慢，甚至可能直接timeout或者报一些其他错误。我们可以根据错误来优化maven中jetty的启动速度。 常见错误一当遇到类似如下错误： java.lang.ArrayIndexOutOfBoundsException: 51889 或者： java.lang.Exception: Timeout scanning annotations 解决办法在web.xml中的web-app标签设置属性metadata-complete=”true” &lt;web-app xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; version=&quot;3.0&quot; metadata-complete=&quot;true&quot;&gt; 产生原因官方原因解释如下： One important thing to be aware of is that the 2.5 Servlet Specification has introduced a new attribute into the element, the metadata-complete attribute. If true, then the web container will NOT search the webapp for source code annotations, and your web.xml file must contain all resources required. If false or not specified, then jetty is required to examine all servlets, filters and listeners in the webapp for annotations. Therefore, you can save startup time by using this attribute correctly - if you don’t want to use annotations then ensure you mark metadata-complete=”true”, otherwise you will pay the penalty of the code examination. 也就是说如果不设置metadata-complete=”true”，那么jetty会检查程序中所有的annotations，而程序中spring和其他的annotations是不需要jetty来检查的。 常见错误二出现如下提示信息： [INFO] No Transaction manager found - if your webapp requires one, please configure one. 解决办法首先修改pom.xml中jetty插件的配置： &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-maven-plugin&lt;/artifactId&gt; &lt;version&gt;9.3.0.M1&lt;/version&gt; &lt;configuration&gt; &lt;httpConnector&gt; &lt;port&gt;8888&lt;/port&gt; &lt;/httpConnector&gt; &lt;!-- 本地装载contextXml，来解决未配置事务或数据库造成启动时等待时间过长 --&gt; &lt;contextXml&gt;src/main/resources/jetty-deploy.xml&lt;/contextXml&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; 关键是新增了contextXml项的配置，jetty-deploy.xml具体内容如下： &lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE Configure PUBLIC &quot;-//Jetty//Configure//EN&quot; &quot;http://www.eclipse.org/jetty/configure.dtd&quot;&gt; &lt;!-- =============================================================== --&gt; &lt;!-- Add a ContextProvider to the deployment manager --&gt; &lt;!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --&gt; &lt;!-- This scans the webapps directory for war files and directories --&gt; &lt;!-- to deploy. --&gt; &lt;!-- This configuration must be used with jetty-deploy.xml, which --&gt; &lt;!-- creates the deployment manager instance --&gt; &lt;!-- =============================================================== --&gt; &lt;Configure id=&quot;Server&quot; class=&quot;org.eclipse.jetty.webapp.WebAppContext&quot;&gt; &lt;Call name=&quot;setAttribute&quot;&gt; &lt;Arg&gt;org.eclipse.jetty.server.webapp.WebInfIncludeJarPattern&lt;/Arg&gt; &lt;Arg&gt;.*/mwa-web-.*\.jar$&lt;/Arg&gt; &lt;!--&lt;Arg&gt;.*/.*jsp-api-[^/]\.jar$|./.*jsp-[^/]\.jar$|./.*taglibs[^/]*\.jar$&lt;/Arg&gt;--&gt; &lt;/Call&gt; &lt;/Configure&gt; 产生原因项目中未配置事务或数据库造成启动时等待时间过长。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java锁的种类及研究]]></title>
      <url>%2F2016%2F01%2F13%2Fjava-lock-research%2F</url>
      <content type="text"><![CDATA[背景锁作为并发共享数据，保证一致性的工具，在JAVA平台有多种实现(如 synchronized 和 ReentrantLock等等 ) 。这些已经写好提供的锁为我们开发提供了便利，但是锁的具体性质以及类型却很少被提及。 自旋锁自旋锁是采用让当前线程不停地的在循环体内执行实现的，当循环的条件被当前线程改变时其他前程才能进入临界区。 自旋锁流程：获取自旋锁时，如果没有任何线程保持该锁，那么将立即得到锁；如果在获取自旋锁时锁已经有保持者，那么获取锁操作将自旋在那里，直到该自旋锁的保持者释放了锁。 简单实现原理的代码如下： /** * 自旋锁原理简单示例 * * @author zacard * @since 2016-01-13 21:40 */ public class SpinLock { private AtomicReference&lt;Thread&gt; sign = new AtomicReference&lt;&gt;(); // 获取锁 public void lock() { Thread current = Thread.currentThread(); while (!sign.compareAndSet(null, current)) { } } // 释放锁 public void unlock() { Thread current = Thread.currentThread(); sign.compareAndSet(current, null); } } 要理解以上代码，我们要先弄清楚AtomicReference的作用。 AtomicReference：位于java.util.concurrent.atomic包下。从包名就可知道它的大致作用：在并发环境中保证引用对象的原子操作。 查看AtomicReference源码： package java.util.concurrent.atomic; import java.util.function.UnaryOperator; import java.util.function.BinaryOperator; import sun.misc.Unsafe; /** * An object reference that may be updated atomically. See the {@link * java.util.concurrent.atomic} package specification for description * of the properties of atomic variables. * @since 1.5 * @author Doug Lea * @param &lt;V&gt; The type of object referred to by this reference */ public class AtomicReference&lt;V&gt; implements java.io.Serializable { private static final long serialVersionUID = -1848883965231344442L; private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicReference.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); } } private volatile V value; ...(省略) /** * Atomically sets the value to the given updated value * if the current value {@code ==} the expected value. * @param expect the expected value * @param update the new value * @return {@code true} if successful. False return indicates that * the actual value was not equal to the expected value. */ public final boolean compareAndSet(V expect, V update) { return unsafe.compareAndSwapObject(this, valueOffset, expect, update); } ...(省略) 发现AtomicReference实现的基本原理是使用volatile关键字和Unsafe类来保证其可见性和原子性。（PS:在此暂不作扩展阅读Unsafe类） 我们重点关注AtomicReference.compareAndSet()这个自旋锁用到的方法。从方法注释和方式实现，可以理解：这个方法的意思就是当当前的值==（注意是双等号）期望的值（即传入的第一个参数）时，把当前值更新为新值（即传入的第二个参数），并且返回true，否则返回false。 再回过头，看之前自旋锁的代码，就很好理解了。一开始AtomicReference中的值为null，当有线程获得锁后，将值更新为该线程。当其他线程进入被锁的方法时，由于sign.compareAndSet(null, current)始终返回的是false，导致while循环体一直在运行，知道获得锁的线程调用unlock方法，将当前持有线程重新设置为null：sign.compareAndSet(current, null)其他线程才可获得锁。 阻塞锁阻塞锁，与自旋锁不同，改变了线程的运行状态。阻塞锁，可以说是让线程进入阻塞状态进行等待，当获得相应的信号（唤醒，时间） 时，才可以进入线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态。 阻塞锁和自旋锁最大的区别就在于，当获取锁是，如果锁有持有者，当前线程是进入阻塞状态，等待当前线程结束而被唤醒的。 简单实现原理的代码如下： /** * 阻塞锁原理简单示例 * * @author zacard * @since 2016-01-13 22:02 */ public class BlockLock { private AtomicReference&lt;Thread&gt; sign = new AtomicReference&lt;&gt;(); // 获取锁 public void lock() { Thread current = Thread.currentThread(); if (!sign.compareAndSet(null, current)) { LockSupport.park(); } } // 释放锁 public void unlock() { Thread current = Thread.currentThread(); sign.compareAndSet(null, current); LockSupport.unpark(current); } } 要理解以上代码，我们要先弄清楚LockSupport的作用。 LockSupport：位于java.util.concurrent.locks包下（又是j.u.c）。同样，从包名和类名即可知道其作用：提供并发编程中的锁支持。 还是先查看下LockSupport的源码: public class LockSupport { private LockSupport() {} // Cannot be instantiated. private static void setBlocker(Thread t, Object arg) { // Even though volatile, hotspot doesn&apos;t need a write barrier here. UNSAFE.putObject(t, parkBlockerOffset, arg); } ...(省略) 又是sun.misc.Unsafe这个类，在此我们不得不先扩展研究下这个Unsafe类的作用和原理了。 sun.misc.Unsafe：有个称号叫做魔术类。因为他能直接操作内存等一些复杂操作。包括直接修改内存值，绕过构造器，直接调用类方法等。当然，他主要提供了CAS（compareAndSwap）原子操作而被我们熟知。 查看Unsafe类源码: public final class Unsafe { private static final Unsafe theUnsafe; ...(省略) private Unsafe() { } @CallerSensitive public static Unsafe getUnsafe() { Class var0 = Reflection.getCallerClass(); if(!VM.isSystemDomainLoader(var0.getClassLoader())) { throw new SecurityException(&quot;Unsafe&quot;); } else { return theUnsafe; } } ...(省略) 根据代码可知：Unsafe是final类，意味着我们不能通过继承来使用或改变这个类的方法。然后构造器是私有的，也不能实例化。但是他自己保存了一个静态私有不可改变的实例“theUnsafe”，并且只提供了一个静态方法getUnsafe()来获取这个类的实例。 但是这个getUnsafe方法确有个限制：注意if语句里的判断，他表示如果不是受信任的类调用，会直接抛出异常。显然，我们平常编写的类都是不受信任的！ 但是，我们有反射！既然他已经持有了一个实例，就能通过反射强行窃取这个私有的实例。 代码如下: public void getUnsafe() { try { Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); field.setAccessible(true); Unsafe unsafe = (Unsafe) field.get(null); } catch (NoSuchFieldException | IllegalAccessException e) { e.printStackTrace(); } } Unsafe类的方法基本都是native关键字修饰的，也就是说这些方法都是原生态方法，方法对应的实现不是在当前文件，而是在用其他语言（如C和C++）实现的文件中。这也就是为什么Unsafe能够直接操作内存等一些特权功能的原因。 回过头看下LockSupport中park()和uppark()这2个方法的作用。 LockSupport.unpark(): /** * Makes available the permit for the given thread, if it * was not already available. If the thread was blocked on * {@code park} then it will unblock. Otherwise, its next call * to {@code park} is guaranteed not to block. This operation * is not guaranteed to have any effect at all if the given * thread has not been started. * * @param thread the thread to unpark, or {@code null}, in which case * this operation has no effect */ public static void unpark(Thread thread) { if (thread != null) UNSAFE.unpark(thread); } 根据方法注释：对于给定线程，将许可证设置为可用状态。如果这个线程是因为调用park()而处于阻塞状态，则清除阻塞状态。反之，这个线程在下次调用park()时，将保证不被阻塞。 LockSupport.park()： /** * Disables the current thread for thread scheduling purposes unless the * permit is available. * * &lt;p&gt;If the permit is available then it is consumed and the call returns * immediately; otherwise * the current thread becomes disabled for thread scheduling * purposes and lies dormant until one of three things happens: * * &lt;ul&gt; * &lt;li&gt;Some other thread invokes {@link #unpark unpark} with the * current thread as the target; or * * &lt;li&gt;Some other thread {@linkplain Thread#interrupt interrupts} * the current thread; or * * &lt;li&gt;The call spuriously (that is, for no reason) returns. * &lt;/ul&gt; * * &lt;p&gt;This method does &lt;em&gt;not&lt;/em&gt; report which of these caused the * method to return. Callers should re-check the conditions which caused * the thread to park in the first place. Callers may also determine, * for example, the interrupt status of the thread upon return. * * @param blocker the synchronization object responsible for this * thread parking * @since 1.6 */ public static void park(Object blocker) { Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null); } 根据注释：除非许可证是可用的，不然将当前线程的调度设置为不可用。当许可是可用时，方法会立即返回，不会阻塞，反之就会阻塞当前线程直到下面3件事发生: 其他线程调用了unpark(此线程) 其他线程interrupts（终止）了此线程 调用时发生未知原因的返回 重入锁重入锁，也叫做递归锁，指的是同一线程外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。在JAVA环境下ReentrantLock和synchronized都是重入锁。 测试代码如下: /** * 测试ReentrantLock和synchronized */ @Test public void testReentrantLock() { // ReentrantLock test for (int i = 0; i &lt; 3; i++) { new Thread(new Runnable() { ReentrantLock lock = new ReentrantLock(); public void get() { lock.lock(); System.out.println(&quot;ReentrantLock:&quot; + Thread.currentThread().getId()); set(); lock.unlock(); } public void set() { lock.lock(); System.out.println(&quot;ReentrantLock:&quot; + Thread.currentThread().getId()); lock.unlock(); } @Override public void run() { get(); } }).start(); } // synchronized test for (int i = 0; i &lt; 3; i++) { new Thread(new Runnable() { public synchronized void get() { System.out.println(&quot;synchronized:&quot; + Thread.currentThread().getId()); set(); } public synchronized void set() { System.out.println(&quot;synchronized:&quot; + Thread.currentThread().getId()); } @Override public void run() { get(); } }).start(); } } 2段代码的输出一致：都会重复输出当前线程id2次。 可重入锁最大的作用是避免死锁。以自旋锁作为例子： /** * 自旋锁原理简单示例 * * @author zacard * @since 2016-01-13 21:40 */ public class SpinLock { private AtomicReference&lt;Thread&gt; sign = new AtomicReference&lt;&gt;(); // 获取锁 public void lock() { Thread current = Thread.currentThread(); while (!sign.compareAndSet(null, current)) { } } // 释放锁 public void unlock() { Thread current = Thread.currentThread(); sign.compareAndSet(current, null); } } 若有同一线程两调用lock()，会导致第二次调用lock位置进行自旋，产生了死锁说明这个锁并不是可重入的。（在lock函数内，应验证线程是否为已经获得锁的线程） 若1问题已经解决，当unlock（）第一次调用时，就已经将锁释放了。实际上不应释放锁 自旋锁避免死锁的方法（采用计数次统计）： /** * 自旋锁改进 * * @author Guoqw * @since 2016-01-14 14:11 */ public class SpinLockImprove { private AtomicReference&lt;Thread&gt; owner = new AtomicReference&lt;&gt;(); private int count = 0; /** * 获取锁 */ public void lock() { Thread current = Thread.currentThread(); if (current == owner.get()) { count++; return; } while (!owner.compareAndSet(null, current)) { } } /** * 释放锁 */ public void unlock() { Thread current = Thread.currentThread(); if (current == owner.get()) { if (count != 0) { count--; } else { owner.compareAndSet(current, null); } } } } 改进后自旋锁即为重入锁的简单实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[单元测试中涉及到mybatis plugin时需要注意的地方]]></title>
      <url>%2F2016%2F01%2F11%2Ftestcase-with-mybatis-plugin%2F</url>
      <content type="text"><![CDATA[背景当编写test case的时候，如果测试内容中又涉及到mybatis plugin（interceptor）的时候，可能会出现奇怪的问题。比如一部分mybatis的dao调用进入了plugin,一小部分却不经过plugin。 例子请看如下单元测试代码： /** * 测试分页查询 */ @Test public void testQueryPage() { // 分页对象 Pager&lt;Account&gt; pager = new Pager&lt;&gt;(); pager.setCurrentPage(2); pager.setPageSize(10); // 查询参数 QueryParams queryParams = new QueryParams(); queryParams.setRole(&quot;ADMIN&quot;); // 调用查询接口 List&lt;Account&gt; accounts1 = accountDAO.queryPage(queryParams, pager); Assert.assertTrue(accounts1.size() == 1); // 调用查询接口 pager.setCurrentPage(1); List&lt;Account&gt; accounts2 = accountDAO.queryPage(queryParams, pager); Assert.assertTrue(accounts2.size() == 10); } mybatis plugin代码： /** * &lt;p&gt; * 数据库分页和排序插件，只拦截查询语句. * &lt;/p&gt; * */ @Intercepts({@Signature(type = StatementHandler.class, method = &quot;prepare&quot;, args = {Connection.class})}) public class QueryInterceptor implements Interceptor{ ... } 这时候，跑测试类，第二次调用dao接口的时候，没有进入mybatis的plugin，断言也是不通过的。这是由于mybatis的一级缓存的机制。 原因当几个查询出于同一个sqlsession查询的时候，并且mybatis认为是完全相同的查询，mybatis是使用一级缓存的结果，而不会走拦截StatementHandler接口的插件方法。（mybatis认为的完全相同的查询，不是指使用sqlsession查询时传递给算起来session的所有参数值完完全全相同，你只要保证statementId，rowBounds,最后生成的SQL语句，以及这个SQL语句所需要的参数完全一致就可以了。） 总结这个问题教育我们，test case的方法要尽量独立，测试用例尽量测试一个业务或者逻辑。 Ps:mybatis一级缓存机制参考这里;mybatis拦截器参考这里]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java list循环中删除元素的坑]]></title>
      <url>%2F2016%2F01%2F07%2Flist-remove%2F</url>
      <content type="text"><![CDATA[背景当我们要循环一个list中的元素，并且要删除某个元素的时候，一点要小心谨慎！其中深埋了好几个坑！ 坑1请看如下代码： /** * 测试删除集合中的空白元素 */ @Test public void removeBlank() { List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;1&quot;); list.add(&quot;2&quot;); list.add(&quot;3&quot;); list.add(&quot; &quot;); list.add(&quot; &quot;); for (String s : list) { if (StringUtils.isBlank(s)) { list.remove(s); } } System.out.println(&quot;list:&quot; + list); } 输出结果：list:[1, 2, 3, ] 。可以看到空白元素没有删除干净。 坑1解决办法请看如下代码： /** * 测试删除集合中的空白元素 */ @Test public void removeBlank() { List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;1&quot;); list.add(&quot;2&quot;); list.add(&quot;3&quot;); list.add(&quot; &quot;); list.add(&quot; &quot;); Iterator&lt;String&gt; iterator = list.iterator(); while (iterator.hasNext()) { String s = iterator.next(); if (StringUtils.isBlank(s)) { iterator.remove(); } } System.out.println(&quot;list:&quot; + list); } 结果输出：list:[1, 2, 3]。解决办法其实就是用Iterator迭代器代替for循环。但是这个解决方法里还是隐藏了一个坑。 坑2请看如下代码： /** * 测试删除集合中的空白元素 */ @Test public void removeBlank() { List&lt;String&gt; list = Arrays.asList(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;&quot;,&quot; &quot;); Iterator&lt;String&gt; iterator = list.iterator(); while (iterator.hasNext()) { String s = iterator.next(); if (StringUtils.isBlank(s)) { iterator.remove(); } } System.out.println(&quot;list:&quot; + list); } 结果会直接报错：java.lang.UnsupportedOperationException。意思是不支持remove操作。只是把list的定义换成了Arrays.asList，却有完全不一样的运行结果，非常神奇。查看Arrays.asList的源码： /** * Returns a fixed-size list backed by the specified array. (Changes to * the returned list &quot;write through&quot; to the array.) This method acts * as bridge between array-based and collection-based APIs, in * combination with {@link Collection#toArray}. The returned list is * serializable and implements {@link RandomAccess}. * * &lt;p&gt;This method also provides a convenient way to create a fixed-size * list initialized to contain several elements: * &lt;pre&gt; * List&amp;lt;String&amp;gt; stooges = Arrays.asList(&quot;Larry&quot;, &quot;Moe&quot;, &quot;Curly&quot;); * &lt;/pre&gt; * * @param &lt;T&gt; the class of the objects in the array * @param a the array by which the list will be backed * @return a list view of the specified array */ @SafeVarargs @SuppressWarnings(&quot;varargs&quot;) public static &lt;T&gt; List&lt;T&gt; asList(T... a) { return new ArrayList&lt;&gt;(a); } 单从代码看：return new ArrayList&lt;&gt;(a);应该是一个普通的ArrayList啊？！查看注释：返回一个固定大小的list！也就是说add和remove操作肯定会报错。同时也说明了这里的ArrayList不是我们平时使用的ArrayList。继续跟踪这个ArrayList： /** * @serial include */ private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, java.io.Serializable { private static final long serialVersionUID = -2764017481108945198L; private final E[] a; ArrayList(E[] array) { a = Objects.requireNonNull(array); } ... } 原来此处的ArrayList是Arrays的一个实现了AbstractList的内部类，并且没有覆盖add和remove方法，默认这2个方法是会直接报“UnsupportedOperationException”的。 坑2解决办法既然明白了报错原因，解决办法也很明显了： /** * 测试删除集合中的空白元素 */ @Test public void removeBlank() { List&lt;String&gt; list = Arrays.asList(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;&quot;,&quot; &quot;); List&lt;String&gt; result= new ArrayList&lt;&gt;(list); Iterator&lt;String&gt; iterator = result.iterator(); while (iterator.hasNext()) { String s = iterator.next(); if (StringUtils.isBlank(s)) { iterator.remove(); } } System.out.println(&quot;list:&quot; + result); } 反思通过踩这几个坑，再次验证了一个真理：在设计一个对外方法的时候，一点要谨慎处理集合和数组。因为你永远不知道客户端传给你的集合是什么，也不知道客户端是否会有对此集合有任何其他的不可控的操作。所以在使用客户端传递的集合对象时，最好拷贝一个新集合后再操作。 更新-2016.01.17之前的解决方法中违反了java其中的一个编码准则：变量的作用范围越小约好。 请看如下代码： @Test public void testForAndWhile() { List&lt;String&gt; names1 = Lists.newArrayList(&quot;张三&quot;, &quot;李四&quot;); Iterator&lt;String&gt; iterator1= names1.iterator(); while (iterator1.hasNext()) { String name = iterator1.next(); System.out.println(&quot;name1:&quot; + name); iterator1.remove(); } List&lt;String&gt; names2 = Lists.newArrayList(&quot;赵六&quot;, &quot;钱七&quot;); Iterator&lt;String&gt; iterator2= names2.iterator(); while (iterator1.hasNext()) { String name = iterator2.next(); System.out.println(&quot;name2:&quot; + name); iterator2.remove(); } } 这段代码的第二个循环不会打印任何东西，因为while条件中误用了第一个循环中的变量，造成的bug。但是这种bug却不会有任何编译错误和运行时错误，危害甚大。但是只要改成旧版for循环的形式，就可以很好的避免这种错误。 请看如下改进代码： @Test public void testForAndWhile() { List&lt;String&gt; names1 = Lists.newArrayList(&quot;张三&quot;, &quot;李四&quot;); for (Iterator&lt;String&gt; iterator= names1.iterator();iterator.hasNext();) { System.out.println(&quot;name1:&quot;+iterator.next()); iterator.remove(); } List&lt;String&gt; names2 = Lists.newArrayList(&quot;赵六&quot;, &quot;钱七&quot;); for (Iterator&lt;String&gt; iterator= names2.iterator();iterator.hasNext();) { System.out.println(&quot;name2:&quot;+iterator.next()); iterator.remove(); } } 这个时候，如果错误的引用循环1中的代码，会有编译时错误，就可以很好的避免这种问题、]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于windows下jetty锁文件的解决办法]]></title>
      <url>%2F2016%2F01%2F06%2Fjetty-lock-file%2F</url>
      <content type="text"><![CDATA[在windows环境中开发，当使用jetty作为容器时，可能会发生修改js、css文件而没有生效，甚至报错的情况。 产生原因这是因为jetty会使用内存映射文件来缓存静态文件，其中就包括js、css文件。而在windows下，使用内存映射文件会导致文件被锁定。 解决方法解决方法很简单，只需要再web.xml文件中配置相应的静态文件不使用内存映射文件来缓存即可： &lt;!--配置使windows下的jetty不锁定文件--&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;/static/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;init-param&gt; &lt;param-name&gt;useFileMappedBuffer&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java enum的妙用]]></title>
      <url>%2F2015%2F12%2F14%2Fjava-enum%E7%9A%84%E5%A6%99%E7%94%A8%2F</url>
      <content type="text"><![CDATA[java中的枚举，大家应该并不陌生。可enum的特性和用法，可能并不是很了解。enum的出现主要是为了代替public static final的常量的。因为常量有诸多的不便，包括没有命名空间，无法按组隔离，使用容易遗忘，扩展不便，没有编译时错误约束等。于是enum出现了，本文将介绍一些enum巧妙的用法。 enum的特点enum具有以下特点： 单例，java中实现单例最快捷的方式 本质上是一个final类，并且继承java.lang.Enum 可以在switch判断语句上使用enum,但是并不推荐 可以设置具体的枚举值 利用抽象类构造符合开闭原则的代码当我们使用enum的时候，大部分人会用switch语句做判断而实现不同枚举使用不用的逻辑。例如以下一个计算器操作的例子： /** * 计算器操作枚举 * * @author zacard * @since 2015-12-14 10:17 */ public enum Operation { PLUS(&quot;+&quot;), MINUS(&quot;-&quot;), MULTIPLY(&quot;*&quot;), DIVISION(&quot;/&quot;); private String symbol; Operation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } } /** * 测试 * * @author zacard * @since 2015-12-14 10:12 */ public class MyTest { // 计算 public int apply(Operation operation, int x, int y) { int result; switch (operation) { case PLUS: result = x + y; break; case MINUS: result = x - y; break; case MULTIPLY: result = x * y; break; case DIVISION: result = x / y; break; default: result = 0; break; } return result; } } 这是一个很常见的例子，但是确有极大的隐患。比如计算器操作类增加一个枚举：RADICAL(“√￣”)根号，很多客户端的代码就需要改，而且很容易遗忘，因为没有编译时错误的约束。并且大量的switch case语句使得代码冗余，更不符合开闭原则。 只需要对枚举类稍加改造，增加一个抽象方法，即可改变以上种种的问题，代码如下： /** * 计算器操作枚举 * * @author zacard * @since 2015-12-14 10:17 */ public enum Operation { PLUS(&quot;+&quot;) { @Override public int apply(int x, int y) { return x + y; } }, MINUS(&quot;-&quot;) { @Override public int apply(int x, int y) { return x - y; } }, MULTIPLY(&quot;*&quot;) { @Override public int apply(int x, int y) { return x * y; } }, DIVISION(&quot;/&quot;) { @Override public int apply(int x, int y) { return x / y; } }, // 新增加的操作，开根号 RADICAL(&quot;√￣&quot;) { @Override public int apply(int x, int y) { return (int) Math.sqrt(x); } }; private String symbol; Operation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } // 计算 public abstract int apply(int x, int y); } /** * 测试 * * @author zacard * @since 2015-12-14 10:12 */ public class MyTest { // 计算 public int apply(Operation operation, int x, int y) { return operation.apply(x, y); } } 当定义了一个抽象方法apply的时候，枚举必须实现这个抽象方法，不然会编译报错，不会造成新增加一个枚举而造成逻辑遗漏的问题。同时，客户端调用的代码页变得异常简单和优雅。 利用接口实现可伸缩的枚举类我们知道枚举本质是继承Enum的类，并且java是单继承的。因此我们可以使用接口，让枚举实现接口，来构建可伸缩的枚举类，代码如下： /** * 计算器操作动作 * * @author Guoqw * @since 2015-12-14 13:09 */ public interface Action { // 计算 int apply(int x, int y); } /** * @author Guoqw * @since 2015-12-14 10:17 */ public enum Operation implements Action { PLUS(&quot;+&quot;) { @Override public int apply(int x, int y) { return x + y; } }, MINUS(&quot;-&quot;) { @Override public int apply(int x, int y) { return x - y; } }, MULTIPLY(&quot;*&quot;) { @Override public int apply(int x, int y) { return x * y; } }, DIVISION(&quot;/&quot;) { @Override public int apply(int x, int y) { return x / y; } }, RADICAL(&quot;√￣&quot;) { @Override public int apply(int x, int y) { return (int) Math.sqrt(x); } }; private String symbol; Operation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } } 这样的好处是当我们要扩充计算器操作枚举类是，来的很方便，代码如下： /** * 计算器操作扩展类 * * @author Guoqw * @since 2015-12-14 13:13 */ public enum ExtendOperation implements Action { // 倒数 RECIPROCAL(&quot;1/x&quot;) { @Override public int apply(int x, int y) { return 1 / x; } }; private String symbol; ExtendOperation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用log4jdbc更有效的记录java sql日志]]></title>
      <url>%2F2015%2F09%2F24%2Flog4jdbc20150924%2F</url>
      <content type="text"><![CDATA[Log4jdbc 是一个开源 SQL 日志框架，它使用代理模式实现对常用的 JDBC Driver（ Oracle , Derby , MySQL , PostgreSQL , H2 , HSQLDB , …）操作的拦截，记录真实 SQL ，可以将占位符与参数全部合并在一起显示，方便直接拷贝 SQL 在 MySQL 等客户端直接执行，加快调试速度。 Log4jdbc的特点Log4jdbc具有以下特点: 完全支持 JDBC3 和 JDBC4 配置简单，在大多数情况下，只需要将 JDBC 驱动类改成net.sf.log4jdbc.DriverSpy ，同时将 jdbc:log4jdbc 添加到现有的 JDBC URL 之前，最后配置日志记录的种类即可 将 Prepared Statements 中的绑定参数自动插入到对应的位置。在大多数情况下极大改善了可读性及调试工作 SQL 的耗时信息能被获取从而帮助判断哪些语句执行得过慢，同时这些信息可以被工具识别得到一个关于慢 SQL 的报表 SQL 连接信息也可以获取从而帮助诊断关于连接池或线程的问题 兼容任何 JDBC 驱动，需要 JDK1.4 及以上与 Slf4j1.x 开源软件，使用 Apache 2.0 License 使用步骤决定使用哪个版本的jar包JDK1.5:log4jdbc-log4j2-jdbc3.jar JDK1.6:log4jdbc-log4j2-jdbc4.jar JDK1.7:log4jdbc-log4j2-jdbc4.1.jar JDK1.8:log4jdbc-log4j2-jdbc4.1.jar 将 JAR 包添加进项目这里只介绍maven方式引入 &lt;!--log4jdbc--&gt; &lt;dependency&gt; &lt;groupId&gt;org.bgee.log4jdbc-log4j2&lt;/groupId&gt; &lt;artifactId&gt;log4jdbc-log4j2-jdbc4.1&lt;/artifactId&gt; &lt;version&gt;1.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.5&lt;/version&gt; &lt;/dependency&gt; 修改项目的JDBC驱动类#jdbc.driver=com.mysql.jdbc.Driver jdbc.driver=net.sf.log4jdbc.sql.jdbcapi.DriverSpy 将jdbc:log4添加到现有的JDBC URL之前#jdbc.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;amp;characterEncoding=UTF-8 jdbc.url=jdbc:log4jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;amp;characterEncoding=UTF-8 配置日志记录的种类Log4jdbc 用以下几个可以配置的日志种类： jdbc.sqlonly : 仅记录 SQL jdbc.sqltiming ：记录 SQL 以及耗时信息 jdbc.audit ：记录除了 ResultSet 之外的所有 JDBC 调用信息，会产生大量的记录，有利于调试跟踪具体的 JDBC 问题 jdbc.resultset ：会产生更多的记录信息，因为记录了 ResultSet 的信息 jdbc.connection ：记录连接打开、关闭等信息，有利于调试数据库连接相关问题 以上日志种类都可以设置为 DEBUG , INFO 或 ERROR 级别。当设置为 FATAL 或 OFF 时，意味关闭记录。 以下是一个采用 Log4j 作为具体日志系统的典型配置，将这些配置到 log4j.properties 里面： log4j.logger.jdbc.sqlonly=OFF log4j.logger.jdbc.sqltiming=INFO log4j.logger.jdbc.audit=OFF log4j.logger.jdbc.resultset=OFF log4j.logger.jdbc.connection=OFF 添加log4jdbc.log4j2.properties文件这是最后一步，在项目的 CLASSPATH 路径下创建一个 log4jdbc.log4j2.properties 文件，告诉 Log4jdbc-log4j2 使用的是 Slf4j 来记录和打印日志，在该配置文件里增加： log4jdbc.spylogdelegator.name=net.sf.log4jdbc.log.slf4j.Slf4jSpyLogDelegator 运行项目，查看效果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[反向代理工具ngrok使用说明]]></title>
      <url>%2F2015%2F09%2F17%2Fngrok-use-info%2F</url>
      <content type="text"><![CDATA[由于阿里支付和微信支付需要一个外网网址的回调url，本机测试代码的时候不是很方便。这里因此使用反向代理工具，可以生成一个外网网址，代理本机127.0.0.1和指定端口（包括80）。 经过研究，发现ngrok这个工具使用简单，配置方便且完全免费使用。由于ngrok被墙了，这里国内有人搭建了个类似ngrok的服务：Tunnel 使用说明 配置go语言环境brew install go 安装ngrokbrew install ngrok 运行命令./ngrok -config ngrok.cfg -subdomain [你喜欢的域名] [代理的端口] 例如： ./ngrok -config ngrok.cfg -subdomain zacard 80 查看运行情况出现如下情况： Tunnel Status online 表示代理成功了~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用spring-data-jpa遇到的一个坑]]></title>
      <url>%2F2015%2F09%2F10%2Fspring-data-jpa-hole1%2F</url>
      <content type="text"><![CDATA[项目使用的是mybatis，相比于hibernate，更轻量更简洁。但是有点不好的地方是无法根据entity生成（修改）表。特别是项目丢给测试的时候，开发和测试不是一个数据库。测试往往还要手动根据sql创建一次表。于是考虑在单元测试状态自动生成相关表结构。 方案很显然：spring-data-jpa+hibernate搞定。 配置maven依赖&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;1.8.0.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;4.3.10.Final&lt;/version&gt; &lt;/dependency&gt; 配置spring-data-jpa.xml&lt;!-- Jpa Entity Manager 配置 --&gt; &lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;jpaVendorAdapter&quot; ref=&quot;hibernateJpaVendorAdapter&quot;/&gt; &lt;property name=&quot;packagesToScan&quot; value=&quot;com.test.*.entities&quot;/&gt; &lt;property name=&quot;jpaProperties&quot;&gt; &lt;props&gt; &lt;prop key=&quot;hibernate.ejb.naming_strategy&quot;&gt;org.hibernate.cfg.ImprovedNamingStrategy&lt;/prop&gt; &lt;prop key=&quot;hibernate.hbm2ddl.auto&quot;&gt;update&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置Spring Data的Hibernate接口 --&gt; &lt;bean id=&quot;hibernateJpaVendorAdapter&quot; class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter&quot;&gt; &lt;property name=&quot;database&quot; value=&quot;MYSQL&quot;/&gt; &lt;property name=&quot;showSql&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt; 测试类package com.test.service; import com.alibaba.fastjson.JSON; import com.test.dao.AccountDao; import com.test.entities.Account; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.testng.AbstractTestNGSpringContextTests; import org.testng.annotations.Test; /** * spring-test with testng * @author zacard * @since 2015-09-10 12:53 */ @ContextConfiguration(locations = {&quot;classpath:spring-data-jpa.xml&quot;, &quot;classpath:spring-services.xml&quot;}) public class NormalServiceTest extends AbstractTestNGSpringContextTests { @Autowired private AccountDao accountDao; //测试MBG生成的代码是否能正确运行和jap生成表 @Test public void testMBG() { Account account = accountDao.selectByPrimaryKey(1); System.out.println(&quot;account json:&quot;+ JSON.toJSONString(account)); } } 输出正常，查看数据库表，也生成了。搞定~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于maven中mybatis-generator的使用]]></title>
      <url>%2F2015%2F09%2F09%2Fmybatis-generator%2F</url>
      <content type="text"><![CDATA[之前生成mybatis对应的entity、dao、dao.xml啊都是自己写了一套gui工具生成的。感觉还挺好用的。就是通用性不是很强 = =! 今儿偶尔发现原来官方有生成工具，还挺强大的（可惜没有gui啊）。果断试用下（只试验在maven下的使用）。 ps：参考的官网 操作步骤在pom.xml中添加MBG的插件配置&lt;!--mybatis-generator--&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;configuration&gt; &lt;!--配置文件路径--&gt; &lt;configurationFile&gt;${basedir}/src/main/resources/mybatis/generatorConfig.xml&lt;/configurationFile&gt; &lt;!--打印日志--&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;!--覆盖存在的文件--&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;/plugin&gt; 添加generatorConfig.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot; &gt; &lt;generatorConfiguration&gt; &lt;!-- Class Driver Path --&gt; &lt;classPathEntry location=&quot;/xxx/mysql-connector-java-5.1.35.jar&quot;/&gt; &lt;context id=&quot;context&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;commentGenerator&gt; &lt;!-- This property is used to specify whether MBG will include any coments in the generated code --&gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;false&quot;/&gt; &lt;!-- This property is used to specify whether MBG will include the generation timestamp in the generated comments --&gt; &lt;property name=&quot;suppressDate&quot; value=&quot;true&quot;/&gt; &lt;/commentGenerator&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;amp;characterEncoding=UTF-8&quot; userId=&quot;test&quot; password=&quot;123456&quot;/&gt; &lt;javaTypeResolver&gt; &lt;!-- This property is used to specify whether MyBatis Generator should force the use of java.math.BigDecimal for DECIMAL and NUMERIC fields, rather than substituting integral types when possible --&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot;/&gt; &lt;/javaTypeResolver&gt; &lt;javaModelGenerator targetPackage=&quot;com.xkeshi.entities&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;!-- This property is used to select whether MyBatis Generator will generate different Java packages for the objects based on the catalog and schema of the introspected table --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot;/&gt; &lt;!-- This property is used to select whether MyBatis Generator adds code to trim the white space from character fields returned from the database --&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;false&quot;/&gt; &lt;!-- 是否对model添加 构造函数 --&gt; &lt;property name=&quot;constructorBased&quot; value=&quot;true&quot;/&gt; &lt;/javaModelGenerator&gt; &lt;sqlMapGenerator targetPackage=&quot;com.xkeshi.dao&quot; targetProject=&quot;src/main/resources&quot;&gt; &lt;!-- This property is used to select whether MyBatis Generator will generate different Java packages for the objects based on the catalog and schema of the introspected table --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot;/&gt; &lt;/sqlMapGenerator&gt; &lt;javaClientGenerator targetPackage=&quot;com.xkeshi.dao&quot; targetProject=&quot;src/main/java&quot; type=&quot;XMLMAPPER&quot;&gt; &lt;!-- This property is used to select whether MyBatis Generator will generate different Java packages for the objects based on the catalog and schema of the introspected table --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot;/&gt; &lt;/javaClientGenerator&gt; &lt;table tableName=&quot;account&quot; enableCountByExample=&quot;false&quot; enableDeleteByExample=&quot;false&quot; enableSelectByExample=&quot;false&quot; enableUpdateByExample=&quot;false&quot;&gt; &lt;!--insert时id设置--&gt; &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;MySql&quot; identity=&quot;true&quot; type=&quot;pre&quot;/&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt; 输入命令运行1$ mvn mybatis-generator:generate -e 这里-e参数是为了输出错误信息，方便排查问题。看到“BUILD SUCCESS”表示成功生成了~]]></content>
    </entry>

    
  
  
</search>
